{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Trends in AQL\n",
    "\n",
    "In this notebook, we inspect the most prominent queries with respect to certain time windows and compare them to the most frequent queries from google in that same time window.\n",
    "## 1. Annual Analysis\n",
    "First, we look at the most frequent queries in a year. We examine the years from 1999 up to 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the annual top 25 queries from google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "annual_google_trends = pd.read_csv(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/google_trends/google_trends_total.csv\")\n",
    "\n",
    "# print(annual_google_trends['year'].unique())\n",
    "# print(annual_google_trends.columns)\n",
    "# for i in range(2004, 2021):\n",
    "    # print(i, annual_google_trends[annual_google_trends['year'] == i])\n",
    "# print(annual_google_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the annual top 25 queries, the annual top 25 english queries and the annual top 25 google queries from the aql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_aql_trends = pd.read_parquet(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-annual-top-queries-special\")\n",
    "annual_aql_trends_eng = pd.read_parquet(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-annual-top-queries-english\")\n",
    "annual_aql_trends_google = pd.read_parquet(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-annual-top-queries-google\")\n",
    "\n",
    "\n",
    "annual_aql_trends.rename(columns={'serp_query_text_url': 'query', 'count()': 'score'}, inplace=True)\n",
    "annual_aql_trends_eng.rename(columns={'serp_query_text_url': 'query', 'count()': 'score'}, inplace=True)\n",
    "annual_aql_trends_google.rename(columns={'serp_query_text_url': 'query', 'count()': 'score'}, inplace=True)\n",
    "# print(annual_aql_trends.columns)\n",
    "print(annual_aql_trends['score'].sum())\n",
    "aql_top_queries = {}\n",
    "aql_top_queries.update({'aql_google': annual_aql_trends_google})\n",
    "aql_top_queries.update({'aql_english': annual_aql_trends_eng})\n",
    "aql_top_queries.update({'aql_native': annual_aql_trends})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out if we have intersections in the annual top 25 of both query logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform counts into scores by assigning a score of 100 to the most frequent query of each year and compute the ratio of the other queries\n",
    "sizes = {}\n",
    "for key, data in aql_top_queries.items():\n",
    "    print(f\"AQL {key.upper()}:\")\n",
    "    years = set(list(data['year']))\n",
    "    sizes.update({f\"{key}_count\": [data['score'].sum(), 100*data['score'].sum()/346310968]})\n",
    "    for year in years:\n",
    "        # print(year)\n",
    "        max_count = data[data['year'] == year]['score'].max()\n",
    "        data.loc[data['year'] == year, 'score'] = round(data.loc[data['year'] == year, 'score'] / max_count * 100)\n",
    "        # print(year)\n",
    "\n",
    "    years = set(list(data['year']))\n",
    "    for year in years:\n",
    "        if year >= 2004:\n",
    "            aql_queries = set(data[data['year'] == year]['query'].reset_index(drop=True))\n",
    "            google_queries = set(annual_google_trends[annual_google_trends['year'] == year]['query'].reset_index(drop=True))\n",
    "            # google_queries_set = set(google_queries)\n",
    "            # find matching queries\n",
    "            matches = google_queries.intersection(aql_queries)\n",
    "            # print the matching queries\n",
    "            print(f\"{year}: {matches}\")\n",
    "\n",
    "for key,value in sizes.items():\n",
    "    print(f\"{key}: {value}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are only few intersections in the annual top 25 queries of google and the aql queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monthly Analysis\n",
    "\n",
    "We have loaded the top 25 queries of each month from 2005 until 2022 from google. In this analysis we want to compare the actual courses of query frequencies from the AQL and Google. For this, we first need to make a selection which queries we want for that analysis. We take the top 25 queries of the most frequent monthly queries from google. This set is obtained by applying [reciprocal rank fusion](https://dl.acm.org/doi/abs/10.1145/1571941.1572114) to the list of the top 25 monthly queries from 2005 - 2022 by google. \n",
    "First, we load google trends data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# this function groups the data by query and computes the reciprocal rank fusion score for each query \n",
    "def get_rrf_score(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.groupby('query').agg(rrf_score=pd.NamedAgg(column='rank', aggfunc=lambda x: np.sum(1/(60+x)))).reset_index()\n",
    "    return data.sort_values('rrf_score', ascending=False)\n",
    "\n",
    "base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/google_trends/monthly\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for path in base_path.iterdir():\n",
    "    if path.is_file():\n",
    "        df = pd.read_csv(path)\n",
    "        data = pd.concat([data, df])\n",
    "\n",
    "print(data)\n",
    "print(data.shape)\n",
    "print(list(get_rrf_score(data)['query'].head(50)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have collected the top 100 monthly queries from google, we also have retrieved them in the AQL and grouped by their timestamp (in YYYY-MM). Let's load the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "top_25 = ['google', 'yahoo', 'weather', 'youtube', 'hotmail', 'facebook', 'gmail', 'news', 'you', 'ebay', 'amazon', 'games', 'free', 'twitter', 'translate', 'mp3', 'maps', 'msn', 'fb', 'mail', 'instagram', 'map', 'face', 'video', 'juegos']\n",
    "monthly_aql_trends = pd.read_parquet(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-top-monthly-queries-over-time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename col to 'query'\n",
    "monthly_aql_trends = monthly_aql_trends.rename(columns={'serp_query_text_url': 'query'})\n",
    "\n",
    "# get top 25\n",
    "monthly_aql_trends25 = monthly_aql_trends[monthly_aql_trends['query'].isin(top_25)]\n",
    "\n",
    "# convert str to datetime\n",
    "monthly_aql_trends25['time'] = pd.to_datetime(monthly_aql_trends25['time'])\n",
    "\n",
    "# split the data in terms of queries\n",
    "aql_google_data = {}\n",
    "for query in top_25:\n",
    "    aql_google_data.update({query: monthly_aql_trends25[monthly_aql_trends25['query'] == query]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Before computing the correlation, we need to undertake a few steps to make the data comparable. \n",
    "\n",
    "1. Fill empty time slots with the count 0\n",
    "2. Compute the query popularity by taking the ratio of a query's count at a given time and the overall count of google queries in the AQL at that time.\n",
    "3. Project the time series to the interval [0,100].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fill empty months with 0\n",
    "\n",
    "# for each query, fill the missing months between 2004 and 2021 with 0\n",
    "for query in aql_google_data.keys():\n",
    "    # create a date range between 2004 and 2021\n",
    "    date_range = pd.date_range(start='2004-01-01', end='2022-12-01', freq='MS')\n",
    "    # create a new dataframe with the date range\n",
    "    new_df = pd.DataFrame({'time': date_range})\n",
    "    # merge the new dataframe with the original dataframe\n",
    "    aql_google_data[query] = pd.merge(new_df, aql_google_data[query], on='time', how='left')\n",
    "    # fill the missing values with 0\n",
    "    aql_google_data[query]['count()'] = aql_google_data[query]['count()'].fillna(0)\n",
    "    aql_google_data[query]['query'] = aql_google_data[query]['query'].fillna(query)\n",
    "    # convert the time column to datetime\n",
    "    aql_google_data[query]['time'] = pd.to_datetime(aql_google_data[query]['time'])\n",
    "\n",
    "\n",
    "# 2. Compute the monthly score\n",
    "\n",
    "# 2.1 get the monthly count of google queries in the aql\n",
    "monthly_google_data = pd.read_parquet(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-google-queries-monthly-frequency\")\n",
    "monthly_google_data = monthly_google_data.rename(columns={'serp_query_text_url': 'query'})\n",
    "monthly_google_data['time'] = pd.to_datetime(monthly_google_data['time'])\n",
    " # create a date range between 2004 and 2021\n",
    "date_range = pd.date_range(start='2004-01-01', end='2022-12-01', freq='MS')\n",
    "# create a new dataframe with the date range\n",
    "new_df = pd.DataFrame({'time': date_range})\n",
    "monthly_google_data = pd.merge(new_df, monthly_google_data, on='time', how='left')\n",
    "# fill missing values with 1 to avoid division by 0\n",
    "monthly_google_data['count()'] = monthly_google_data['count()'].fillna(1)\n",
    "\n",
    "\n",
    "\n",
    "# 2.2 get relative frequency in aql data by dividing the count by the sum of the count for each month\n",
    "for query in aql_google_data.keys():\n",
    "    # merge the monthly google data with the original dataframe\n",
    "    aql_google_data[query] = pd.merge(aql_google_data[query], monthly_google_data, on='time', how='left', suffixes=('', '_sum'))\n",
    "    # divide the count by the sum of the count for each month\n",
    "    aql_google_data[query]['count()'] = (aql_google_data[query]['count()'] / aql_google_data[query]['count()_sum'])\n",
    "    # drop the count_sum column\n",
    "    aql_google_data[query] = aql_google_data[query].drop(columns=['count()_sum'])\n",
    "\n",
    "\n",
    "# 3. project to [0,100]\n",
    "for query in aql_google_data.keys():\n",
    "    # get the max count\n",
    "    max_count = aql_google_data[query]['count()'].max()\n",
    "    # project to [0,100]\n",
    "    aql_google_data[query]['count()'] = aql_google_data[query]['count()'] / max_count * 100\n",
    "    # round to 2 decimal places\n",
    "    aql_google_data[query]['count()'] = aql_google_data[query]['count()'].round(2)\n",
    "    print(f\"Query: {query}, Max: {aql_google_data[query]['count()'].max()}, Min: {aql_google_data[query]['count()'].min()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to compute temporal correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "# Load the frequencies from google trends data\n",
    "df_time_series = pd.read_csv(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/google_trends/time-series/google_trends_total.csv\")\n",
    "\n",
    "\n",
    "#1. sort the data by time\n",
    "for query in aql_google_data.keys():\n",
    "    aql_google_data[query].sort_values(by=['time'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_temp_corr(time1: pd.Series, time2: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Get the correlation between two time series\n",
    "    :param time1: first time series\n",
    "    :param time2: second time series\n",
    "    :return: correlation between the two time series\n",
    "    \"\"\"\n",
    "    time1 = time1.to_numpy()\n",
    "    time2 = time2.to_numpy()\n",
    "    mean1 = time1.mean()\n",
    "    mean2 = time2.mean()\n",
    "    std1 = time1.std()\n",
    "    std2 = time2.std()\n",
    "\n",
    "    time1 = (time1 - mean1) / std1\n",
    "    time2 = (time2 - mean2) / std2\n",
    "\n",
    "    return np.dot(time1, time2) / len(time1)\n",
    "\n",
    "results = {}\n",
    "# we now iterate through the time series and compute the correlation between the two time series\n",
    "for query in aql_google_data.keys():\n",
    "    print(f\"Query: {query}\")\n",
    "    # get the correlation between the two time series\n",
    "    aql_time_series = aql_google_data[query]['count()']\n",
    "    google_time_series = df_time_series.query(f\"query == '{query}'\")['score']\n",
    "    # print(google_time_series.mean())\n",
    "    # print(aql_time_series.mean())\n",
    "\n",
    "    # print(google_time_series)\n",
    "    # print(aql_time_series)\n",
    "    corr1 = aql_time_series.corr(google_time_series)\n",
    "    corr2 = get_temp_corr(aql_time_series, google_time_series)\n",
    "\n",
    "    # print(f\"Correlation {query}: {corr1}\")\n",
    "    # print(f\"Correlation {query}: {corr2}\")\n",
    "    results.update({query: corr2})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the query names with 4 highest correlations\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "# queries = [query for query, _ in sorted_results[0:4]]\n",
    "queries = [query for query, _ in sorted_results[4:13]]\n",
    "\n",
    "print(queries)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    # get the correlation between the two time series\n",
    "    aql_time_series = aql_google_data[query]['time']\n",
    "    google_time_series = pd.to_datetime(df_time_series.query(f\"query == '{query}'\")['time'])\n",
    "    # print(f\"Type AQL: {type(aql_time_series)}\\nType Google: {type(google_time_series)}\")\n",
    "    # print(aql_google_data[queries[0]].isnull().sum())\n",
    "    # print(df_time_series.isnull().sum())\n",
    "    # print(aql_time_series.head())\n",
    "    # print(google_time_series.head())\n",
    "    print(type(aql_time_series.iloc[0]))\n",
    "    print(type(google_time_series.iloc[0]))\n",
    "    # print(aql_time_series.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the Time series of the AQL and Google with the highest correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "from pathlib import Path\n",
    "\n",
    "# Extract the query names with 4 highest correlations\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "queries = [query for query, _ in sorted_results[4:13]]\n",
    "# queries = [query for query, _ in sorted_results[0:4]]\n",
    "print(queries)\n",
    "\n",
    "save_vis = False\n",
    "# save_vis = True\n",
    "# plot_format = 'pgf'\n",
    "plot_format = 'pdf'\n",
    "\n",
    "analysis_name = \"time-series-google-trends\"\n",
    "\n",
    "# Set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6 / 8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio *0.8*2\n",
    "\n",
    "# Use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"ieee\"])  # \"grid\", \"ieee\",\n",
    "\n",
    "color = ['tab:blue', 'tab:orange']\n",
    "plot_rows = 4\n",
    "plot_cols = 2\n",
    "fig, axes = plt.subplots(ncols=plot_cols, nrows=plot_rows, figsize=(width, height))\n",
    "\n",
    "# Iterate over axes and plot data\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # Get time series\n",
    "    aql_time_series = aql_google_data[queries[i]]['count()']\n",
    "    google_time_series = df_time_series.query(f\"query == '{queries[i]}'\")['score']\n",
    "\n",
    "    # Standardize the time series\n",
    "    # aql_time_series = (aql_time_series - aql_time_series.mean()) / aql_time_series.max()\n",
    "    # google_time_series = (google_time_series - google_time_series.mean()) / google_time_series.std()\n",
    "    # Normalize the time series to [0,100]\n",
    "    aql_time_series = (aql_time_series - aql_time_series.min()) / (aql_time_series.max() - aql_time_series.min()) * 100\n",
    "    google_time_series = (google_time_series - google_time_series.min()) / (google_time_series.max() - google_time_series.min()) * 100\n",
    "\n",
    "    # Plot the time series\n",
    "    ax.plot(aql_google_data[queries[i]]['time'], aql_time_series, label='AQL', color=color[0], alpha=0.5)\n",
    "    ax.plot(pd.to_datetime(df_time_series.query(f\"query == '{queries[i]}'\")['time']), google_time_series, label='Google', color=color[1], alpha=0.5)\n",
    "\n",
    "    # Add query as title\n",
    "    ax.set_title(queries[i], fontweight='bold', fontsize=10)\n",
    "\n",
    "    # Rotate x-axis labels\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # remove xticks if not the last row\n",
    "    if i < (plot_rows-1)*plot_cols:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # Add legend to each subplot\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "\n",
    "# Set global labels\n",
    "# fig.supxlabel('Time', y=0.05)\n",
    "fig.supylabel('Frequency', x=0.03)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Either save visualization or show it\n",
    "plt.show()\n",
    "\n",
    "if save_vis:\n",
    "    vis_dir = Path(f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/{analysis_name}\")\n",
    "    # Make directory if it does not exist\n",
    "    if not vis_dir.exists():\n",
    "        vis_dir.mkdir(parents=True)\n",
    "    # Delete old plot\n",
    "    for file in vis_dir.iterdir():\n",
    "        if file.is_file() and file.name.endswith(f'.{plot_format}'):\n",
    "            file.unlink()\n",
    "\n",
    "    fig.savefig(vis_dir.joinpath(f\"all.{plot_format}\"), format=plot_format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

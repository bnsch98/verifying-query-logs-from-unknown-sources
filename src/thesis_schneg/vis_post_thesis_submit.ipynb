{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e448444d",
   "metadata": {},
   "source": [
    "## Post-Thesis Analysis\n",
    "1. For each comparison log, what percentage of queries (deduplicated) are exactly the same as those contained in the AQL (once case-sensitive, once case-insensitive)?\n",
    "2. What percentage of all queries (deduplicated) are exactly the same as those contained in at least one of the comparison logs (case-sensitive/case-insensitive)?\n",
    "3. Regarding the experiments from 1., what is the situation per year? (The year(s) in which the respective comparison log was crawled is particularly interesting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db801f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from model import AnalysisName, DatasetName\n",
    "from typing import Iterable, Dict, Any\n",
    "\n",
    "def load_results(analysis_name: AnalysisName, dataset_name: DatasetName) -> Iterable[Dict[str, Any]]:\n",
    "    results_folder = f\"{dataset_name}-{analysis_name}\"\n",
    "    if analysis_name in ['count-deduplicated-queries', 'count-deduplicated-lowercase-queries']:\n",
    "        path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/results\")\n",
    "    else:\n",
    "        results_folder += \"-all\"\n",
    "        path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/\")\n",
    "    \n",
    "    path = path.joinpath(results_folder)\n",
    "    assert path.exists(), f\"Path {path} does not exist.\"\n",
    "    # check if files inside path are json or jsonl and load accordingly\n",
    "    files = []\n",
    "    files.extend([file for file in path.glob(\"*.jsonl\")])\n",
    "    files.extend([file for file in path.glob(\"*.json\")])\n",
    "    if not files or all(file.suffix not in [\".jsonl\", \".json\"] for file in files):\n",
    "        raise ValueError(f\"No JSON or JSONL files found in the specified path. Files found: {[file for file in path.glob('*')]}\")\n",
    "    data_list = []\n",
    "    if files[0].suffix == \".jsonl\":\n",
    "        for file in files:\n",
    "            with file.open(\"r\") as f:\n",
    "                data = f.readlines()\n",
    "                for line in data:\n",
    "                    if line.strip():\n",
    "                        line = json.loads(line.strip())\n",
    "                        data_list.append(line)   \n",
    "    elif files[0].suffix == \".json\":\n",
    "        for file in files:\n",
    "            with file.open(\"r\") as f:\n",
    "                data_list.append(json.load(f))\n",
    "    else: \n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "    return data_list\n",
    "\n",
    "analyses = []\n",
    "analyses.append(\"count-regular\")\n",
    "analyses.append(\"count-lowercase\")\n",
    "# analyses.append(\"count-deduplicated-queries\")\n",
    "# analyses.append(\"count-deduplicated-lowercase-queries\")\n",
    "\n",
    "dataset_name = ['aql-aol', 'aql-ms-marco', 'aql-orcas']\n",
    "if \"count-deduplicated-lowercase-queries\" in analyses or \"count-deduplicated-queries\" in analyses:\n",
    "    dataset_name.append('aql-aol-ms-marco-orcas')\n",
    "\n",
    "data_dict = {}\n",
    "for analysis_name in analyses:\n",
    "    data_list = []\n",
    "    data_dict[analysis_name] = data_list\n",
    "\n",
    "\n",
    "    for dataset in dataset_name:\n",
    "        data = load_results(analysis_name, dataset)\n",
    "        data_list.extend(data)\n",
    "\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocess\n",
    "result_dict = {}\n",
    "for analysis_name in analyses:\n",
    "    result_dict[analysis_name] = {}\n",
    "    if analysis_name == \"count-deduplicated-lowercase-queries\" or analysis_name == \"count-deduplicated-queries\":\n",
    "        for i, data in enumerate(data_dict[analysis_name]):\n",
    "            if i < len(data_dict[analysis_name]) - 1:\n",
    "                result_dict[analysis_name][data['dataset_name'][1]] = round(data['intersec_ratio'], 5)\n",
    "            if i == len(data_dict[analysis_name]) - 1:\n",
    "                result_dict[analysis_name]['combined'] = round(data['intersec_ratio_comb'], 5)\n",
    "                # result_dict[analysis_name]['aql_in_combined'] = round(data['intersec_ratio_aql'], 5)\n",
    "    else:\n",
    "        datasets = []\n",
    "        for i,data in enumerate(data_dict[analysis_name]):\n",
    "            if data['dataset'] not in datasets:\n",
    "                result_dict[analysis_name][data['dataset']] = {}\n",
    "                datasets.append(data['dataset'])\n",
    "            if data['year'] >= 1999:\n",
    "                result_dict[analysis_name][data['dataset']][data['year']] = [round(data['duplicate-ratio-aql'][0], 5), round(data['duplicate-ratio-comp'][0], 5)]\n",
    "\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c03418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "import numpy as np\n",
    "\n",
    "assert set(analyses) == set([\"count-deduplicated-lowercase-queries\", \"count-deduplicated-queries\"]), \"Unexpected analyses\"\n",
    "# save_plot = False\n",
    "save_plot = True\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.2\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio * 0.9\n",
    "bar_width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "ax.set_title(\"Percentage of duplicates with the AQL\")\n",
    "ax.set_xlabel(\"Dataset\")\n",
    "ax.set_ylabel(r'Duplicate percentage (\\%)')\n",
    "\n",
    "# Get data for both analyses\n",
    "case_sensitive_data = result_dict[\"count-deduplicated-queries\"]\n",
    "case_insensitive_data = result_dict[\"count-deduplicated-lowercase-queries\"]\n",
    "\n",
    "# Get common datasets\n",
    "datasets = list(case_sensitive_data.keys())\n",
    "\n",
    "# Convert to percentages\n",
    "case_sensitive_values = [case_sensitive_data[dataset] * 100 for dataset in datasets]\n",
    "case_insensitive_values = [case_insensitive_data[dataset] * 100 for dataset in datasets]\n",
    "\n",
    "# Set up bar positions\n",
    "x = np.arange(len(datasets))\n",
    "\n",
    "# Create grouped bars\n",
    "bars1 = ax.bar(x - bar_width/2, case_sensitive_values, bar_width, \n",
    "                label='case sensitive', color='tab:blue', alpha=0.6)\n",
    "bars2 = ax.bar(x + bar_width/2, case_insensitive_values, bar_width, \n",
    "                label='case insensitive', color='tab:orange', alpha=0.6)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars1, case_sensitive_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "            rf'{value:.1f}\\%', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "            \n",
    "for bar, value in zip(bars2, case_insensitive_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "            rf'{value:.1f}\\%', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "# Set x-axis labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([d.upper() for d in datasets])\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_plot:\n",
    "    # save plot as pdf:\n",
    "    folder = analyses[0]\n",
    "    for an in analyses[1:]:\n",
    "        folder += f\"-AND-{an}\"\n",
    "    vis_path =  Path(f\"/home/benjamin/studium/master/masterarbeit/thesis-schneg/plots/{folder}\")\n",
    "    print(vis_path)\n",
    "    if not vis_path.exists():\n",
    "        print(\"create new directory\")\n",
    "        vis_path.mkdir(parents=True)\n",
    "    plt.savefig(vis_path.joinpath(\"plot.pdf\"),format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset we plot the annual duplicate percentage of the AQL with the respective dataset. We do this for both case sensitive and case insensitive duplicates.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "assert set(analyses) == set([\"count-regular\", \"count-lowercase\"]), \"Unexpected analyses\"\n",
    "\n",
    "# save_plot = False\n",
    "save_plot = True\n",
    "\n",
    "datasets = []\n",
    "# datasets.append('aql-aol')\n",
    "datasets.append('aql-ms-marco')\n",
    "# datasets.append('aql-orcas')\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "color =  {\"AQL\":'tab:blue', \"AOL\":'tab:orange', \"MS-MARCO Web Search\":'tab:gray', \"ORCAS\":'tab:red'}\n",
    "\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.2\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio * 0.9\n",
    "bar_width = 0.35\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "    dataset_title = dataset.split('-')[-1].upper() if not \"ms-marco\" in dataset else \"MS-MARCO Web Search\"\n",
    "    ax.set_title(f\"Annual duplicate percentage of AQL with {dataset_title}\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(r\"Duplicate percentage (\\%)\")\n",
    "    \n",
    "    for i, analysis_name in enumerate(analyses):\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        # Here you would add the code to plot the annual duplicate percentage for the dataset\n",
    "        year_nums = [year for year in result_dict[analysis_name][dataset].keys()]\n",
    "        duplicate_ratios = [result_dict[analysis_name][dataset][year][0] for year in year_nums]\n",
    "        \n",
    "        # Create list with first and last day of each year\n",
    "        years = []\n",
    "        duplicate_ratios_expanded = []\n",
    "        for j, year in enumerate(year_nums):\n",
    "            years.append(datetime(year, 1, 1))   # First day of year\n",
    "            years.append(datetime(year, 12, 31)) # Last day of year\n",
    "            # Duplicate the ratio for both first and last day of each year\n",
    "            duplicate_ratios_expanded.append(duplicate_ratios[j] * 100)  # Convert to percentage\n",
    "            duplicate_ratios_expanded.append(duplicate_ratios[j] * 100)  # Convert to percentage\n",
    "        \n",
    "        # Set color based on analysis type\n",
    "        label_text = 'case sensitive' if analysis_name == 'count-regular' else 'case insensitive'\n",
    "        \n",
    "        # Plot the complete line without markers first\n",
    "        alpha_val = 0.6 if analysis_name == 'count-lowercase' else 0.4\n",
    "        ax.plot(years, duplicate_ratios_expanded, \n",
    "               label=label_text, \n",
    "               color=color[dataset_title],\n",
    "               linewidth=1, alpha=alpha_val)\n",
    "\n",
    "        # Add markers only for January 1st dates (every second point in the expanded data)\n",
    "        jan_1_indices = range(0, len(years), 2)  # Every second point (January 1st)\n",
    "        jan_1_years = [years[i] for i in jan_1_indices]\n",
    "        jan_1_ratios = [duplicate_ratios_expanded[i] for i in jan_1_indices]\n",
    "        \n",
    "        # Plot markers only for January 1st\n",
    "        ax.plot(jan_1_years, jan_1_ratios, \n",
    "               color=color[dataset_title],\n",
    "               marker='o', \n",
    "               markersize=4, \n",
    "               markerfacecolor='none',\n",
    "               markeredgecolor=color[dataset_title],\n",
    "               linestyle='None', alpha=alpha_val)  # No line, only markers\n",
    "\n",
    "        print(f\"Years: {years}, Duplicate Ratios: {duplicate_ratios_expanded}\")\n",
    "    \n",
    "    # Set x-axis ticks to show only January 1st of selected years for better readability\n",
    "    all_years = sorted(set(year_nums))\n",
    "    # Show every 2nd or 3rd year depending on data range to avoid overcrowding\n",
    "    step = 2 if len(all_years) <= 10 else 3\n",
    "    selected_years = all_years[::step]\n",
    "    year_ticks = [datetime(year, 1, 1) for year in selected_years]  # January 1st of selected years\n",
    "    ax.set_xticks(year_ticks)\n",
    "    ax.set_xticklabels([str(year) for year in selected_years])\n",
    "    \n",
    "    # Add data collection period as shaded area\n",
    "    collection_periods = {\n",
    "        'aql-aol': (datetime(2006, 3, 1), datetime(2006, 5, 31)),  # March to May 2006\n",
    "        'aql-ms-marco': (datetime(2023, 1, 1), datetime(2024, 1, 1)),  # Approximate period before 2024 publication\n",
    "        'aql-orcas': (datetime(2017, 11, 1), datetime(2020, 1, 31))  # 26-month period up to January 2020\n",
    "    }\n",
    "    \n",
    "    if dataset in collection_periods:\n",
    "        start_date, end_date = collection_periods[dataset]\n",
    "        ax.axvspan(start_date, end_date, alpha=0.2, color='gray', label=f'{dataset_title} query collection period')\n",
    "        \n",
    "        # Add text label at the center of the period\n",
    "        mid_date = start_date + (end_date - start_date) / 2\n",
    "        ax.text(mid_date, ax.get_ylim()[1] * 0.95, f'{dataset_title}\\nquery collection' if dataset_title != \"MS-MARCO Web Search\" else 'MS-MARCO\\nWeb Search\\nquery collection\\n(estimated)', \n",
    "                ha='center', va='top', fontsize=8, color='gray', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7, linewidth=0.5))\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_plot:\n",
    "        # save plot as pdf:\n",
    "        folder = analyses[0]\n",
    "        for an in analyses[1:]:\n",
    "            folder += f\"-AND-{an}\"\n",
    "        vis_path =  Path(f\"/home/benjamin/studium/master/masterarbeit/thesis-schneg/plots/{folder}\")\n",
    "        print(vis_path)\n",
    "        if not vis_path.exists():\n",
    "            print(\"create new directory\")\n",
    "            vis_path.mkdir(parents=True)\n",
    "        plt.savefig(vis_path.joinpath(f\"{dataset_title}.pdf\"),format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

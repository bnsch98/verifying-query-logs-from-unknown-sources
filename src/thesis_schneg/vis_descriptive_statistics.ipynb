{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Descriptive Statistics\n",
    "\n",
    "Of the 4 involved data sets a collection of statistics is created to gain a first insight.\n",
    "For this, we extract named entities, words and characters from the queries. Accordingly, 4 subcategories arise:\n",
    "- queries\n",
    "- named entities\n",
    "- words\n",
    "- characters \n",
    "\n",
    "For each category, the length is measured in terms of all sublevels. E.g. the length of a named entity can be described as the number of words or the number of characters. Alongside, the items of each subcategory are sorted with respect to their frequency to check for specific liguistic laws like zipfs's law. Essentially, we obtain a thorough set of statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Zipf's law \n",
    "Zipf's law states that frequencies of words from a document follow an inversely proportional distribution to their rank when sorted in descending order. To get a more comprehensive comparison between the data sets, we check on zipf's law for all subcategories.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, lets define some configuration for our visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "# combine all plots in one figure or plot each analysis in a single figure\n",
    "single_plot = True\n",
    "# single_plot = False\n",
    "\n",
    "# save visualization or not\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "# select dataset (if none is selected all datasets are visualized in a joint plot) \n",
    "dataset: DatasetName = None \n",
    "# dataset: DatasetName = 'aol'\n",
    "# dataset: DatasetName = 'aql'\n",
    "# dataset: DatasetName = 'ms-marco'\n",
    "# dataset: DatasetName = 'orcas'\n",
    "\n",
    "analyses = []\n",
    "# set analysis that should be visualized\n",
    "analyses.append('query-frequencies')\n",
    "# analyses.append('extract-named-entities')\n",
    "# analyses.append('extract-words')\n",
    "analyses.append('extract-chars')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False\n",
    "\n",
    "take_new_directory = True\n",
    "# take_new_directory = False\n",
    "\n",
    "col = ['count()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the specified data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-query-frequencies-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 1.863358235359192 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.6596764842669169 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-extract-chars-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.18392712672551473 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 1.0721948464711508 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 1.2381206591924032 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.12856982946395873 min\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "    if dataset is None:\n",
    "        result_data = {}\n",
    "        # crawl files from all datasets and load into dictionary\n",
    "        if analysis_name == 'query-frequencies':\n",
    "            paths = {f\"{name}\": _get_results_paths(name, analysis_name, take_new_directory) for name in [\n",
    "                \"aol\", \"aql\"]}\n",
    "        else:\n",
    "            paths = {f\"{name}\": _get_results_paths(name, analysis_name, take_new_directory) for name in [\n",
    "                \"aol\", \"aql\", \"ms-marco\", \"orcas\"]}\n",
    "        # iterate over datasets and create visualization\n",
    "        for name, result_paths in paths.items():\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading data from {name} dataset\")\n",
    "            vis_data = load_results(result_paths, test_data=test_data, cols=col)\n",
    "            result_data.update({name: vis_data})\n",
    "            end_time = time.time()  \n",
    "            print(f\"{name} loaded in {(end_time - start_time)/60} min\")\n",
    "        label=[\"AOL\", \"AQL\", \"MS-MARCO\", \"ORCAS\"]\n",
    "        analysis_data.append(result_data)\n",
    "    else:\n",
    "        # load data from single dataset\n",
    "        result_paths = _get_results_paths(dataset, analysis_name, take_new_directory)\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from {dataset} dataset\")\n",
    "        result_data = {dataset: load_results(result_paths, test_data=test_data, cols=col)}\n",
    "        end_time = time.time()  \n",
    "        print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "        analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing query-frequencies\n",
      "{'dataset-col-x': 'serp_query_text_url', 'dataset-col-y': 'count()', 'x-label': 'Rank', 'y-label': 'Frequency', 'x-lim': None, 'y-lim': None, 'title': \"Zipf's Law Queries\"}\n",
      "Visualizing extract-chars\n",
      "{'dataset-col-x': 'char', 'dataset-col-y': 'count()', 'x-label': 'Rank', 'y-label': 'Frequency', 'x-lim': None, 'y-lim': None, 'title': \"Zipf's Law Characters\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44856/3793448225.py:147: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_44856/3793448225.py:151: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization finished\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import log_plot\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "analyses_params = []\n",
    "for analysis_name in analyses:\n",
    "    # load visualization parametes into dictionary\n",
    "    if analysis_name == 'query-frequencies':\n",
    "        vis_params = {\"dataset-col-x\": \"serp_query_text_url\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Queries\"}\n",
    "    elif analysis_name == 'extract-named-entities':\n",
    "        vis_params = {\"dataset-col-x\": \"entity\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Named Entities\"}\n",
    "    elif analysis_name == 'extract-words':\n",
    "        vis_params = {\"dataset-col-x\": \"word\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Words\"}\n",
    "    elif analysis_name == 'extract-chars':\n",
    "        vis_params = {\"dataset-col-x\": \"char\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Characters\"}\n",
    "    analyses_params.append(vis_params)\n",
    "\n",
    "# latex rendering for matplotlib\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# set path to save visualization\n",
    "if len(analyses) > 1:\n",
    "    if single_plot:\n",
    "        vis_dir = Path(\n",
    "            f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/{analyses[0]}-and-{analyses[1]}-single\")\n",
    "    else:\n",
    "        vis_dir = Path(\n",
    "            f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/{analyses[0]}-and-{analyses[1]}\")\n",
    "else:\n",
    "    if single_plot:\n",
    "        vis_dir = Path(\n",
    "            f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/{analyses[0]}-single\")\n",
    "    else:\n",
    "        vis_dir = Path(\n",
    "            f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/{analyses[0]}\")\n",
    "    \n",
    "# vis_dir = Path(\n",
    "#         f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/extract-named-entities\")\n",
    "\n",
    "# make directory if it does not exist\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "# enable pgf format for matplotlib\n",
    "if save_vis:\n",
    "    matplotlib.use(\"pgf\")\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "# create subplots for each dataset\n",
    "# set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "if single_plot:\n",
    "    fig, ax = plt.subplots(ncols=len(analyses), nrows=1,\n",
    "                           figsize=(width*1.03, 4*height/5))\n",
    "else:\n",
    "    fig, ax = plt.subplots(ncols=len(analyses), nrows=4,\n",
    "                           figsize=(width, 2*height))\n",
    "\n",
    "for i in range(len(analyses)):\n",
    "    result_data = analysis_data[i]\n",
    "    vis_params = analyses_params[i]\n",
    "    print(f\"Visualizing {analyses[i]}\")\n",
    "    print(vis_params)\n",
    "    # create visualization for all data sets if dataset is not specified\n",
    "    if dataset is None:\n",
    "\n",
    "        multi = True\n",
    "        # color palette for datasets\n",
    "        color = ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "        linestyles = [\"solid\", \"dashdot\", \"dashed\", \"dotted\"]\n",
    "        # counter for datasets\n",
    "        cnt_datasets = 0\n",
    "        # iterate over datasets and create visualization\n",
    "        for name, vis_data in result_data.items():\n",
    "            if normalize_data:\n",
    "                vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                    vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "            # apply specific visualization function\n",
    "            if not single_plot:\n",
    "                if len(analyses) > 1:\n",
    "                    fig, ax[cnt_datasets, i] = log_plot(data=vis_data, subplots=(fig, ax[cnt_datasets, i]),\n",
    "                                                        vis_params=vis_params, label=label[cnt_datasets], linestyle=linestyles[cnt_datasets], multi=multi, color=color[cnt_datasets])\n",
    "                else:\n",
    "                    fig, ax[cnt_datasets] = log_plot(data=vis_data, subplots=(fig, ax[cnt_datasets]),\n",
    "                                                     vis_params=vis_params, label=name.upper(), multi=multi, linestyle=linestyles[cnt_datasets], color=color[cnt_datasets])  # , color=color[cnt_datasets]\n",
    "            else:\n",
    "                if len(analyses) > 1:\n",
    "                    fig, ax[i] = log_plot(data=vis_data, subplots=(fig, ax[i]),\n",
    "                                          vis_params=vis_params, label=label[cnt_datasets], multi=multi, linestyle=linestyles[cnt_datasets], color=color[cnt_datasets])\n",
    "                else:\n",
    "                    fig, ax = log_plot(data=vis_data, subplots=(fig, ax),\n",
    "                                       vis_params=vis_params, label=name.upper(), multi=multi, linestyle=linestyles[cnt_datasets], color=color[cnt_datasets])  # , color=color[cnt_datasets]\n",
    "            cnt_datasets += 1\n",
    "\n",
    "    # create visualization for a specific data set:\n",
    "    else:\n",
    "        multi = False\n",
    "\n",
    "        # modify title\n",
    "        vis_params[\"title\"] = f'{vis_params[\"title\"]} ({dataset.upper()})'\n",
    "\n",
    "        # set size of plot\n",
    "        textwidth = 5.5129\n",
    "        aspect_ratio = 6/8\n",
    "        scale = 1.0\n",
    "        width = textwidth * scale\n",
    "        height = width * aspect_ratio\n",
    "\n",
    "        # create subplot for dataset\n",
    "        fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "\n",
    "        # apply specific visualization function\n",
    "        fig, ax = log_plot(data=result_data[dataset], subplots=(fig, ax),\n",
    "                           vis_params=vis_params, multi=multi)\n",
    "\n",
    "        # make layout tight\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # either save visualization or show it\n",
    "        if save_vis:\n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if dataset is None:\n",
    "    fig.supxlabel(analyses_params[0][\"x-label\"], y=0.05)\n",
    "    fig.supylabel(analyses_params[0][\"y-label\"])\n",
    "# make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# either save visualization or show it\n",
    "if save_vis:\n",
    "    fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Length Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the lengths of queries and their previously defined subcategories. For the sake of clarity we plot the results of each data set seperately.\n",
    "\n",
    "Visualization configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "# save visualization or not\n",
    "save_vis: bool = False\n",
    "# save_vis: bool = True\n",
    "\n",
    "# select dataset (if none is selected all datasets are visualized in a joint plot) \n",
    "dataset: DatasetName = None \n",
    "# dataset: DatasetName = 'aol'\n",
    "# dataset: DatasetName = 'aql'\n",
    "# dataset: DatasetName = 'ms-marco'\n",
    "# dataset: DatasetName = 'orcas'\n",
    "\n",
    "# plot cleaned aol data or not\n",
    "cleaned_aol = False\n",
    "# cleaned_aol = True\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('queries')\n",
    "struc_level.append('named-entities')\n",
    "# struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "# base_analyses.append('character-count-frequencies') #-{struc}\n",
    "base_analyses.append('word-count-frequencies')\n",
    "# base_analyses.append('entity-count-frequencies')\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        analyses.append(f'{item}-{struc}')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False\n",
    "\n",
    "# take_new_directory = True\n",
    "take_new_directory = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "analyses = []\n",
    "analyses.append('character-count-frequencies-words') #-{struc}\n",
    "analyses.append('entity-count-frequencies-queries') #-{struc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from aol dataset\n",
      "aol loaded in 0.039054735501607256 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.04401097297668457 min\n",
      "Loading data from ms-marco dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m result_data\u001b[38;5;241m.\u001b[39mupdate({name: vis_data})\n\u001b[1;32m     29\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \n",
      "File \u001b[0;32m~/studium/masterarbeit/thesis-schneg/src/thesis_schneg/vis_modules.py:76\u001b[0m, in \u001b[0;36mload_results\u001b[0;34m(result_files, cols, test_data, filter_rows)\u001b[0m\n\u001b[1;32m     74\u001b[0m         result \u001b[38;5;241m=\u001b[39m pd_read_parquet(result_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         result \u001b[38;5;241m=\u001b[39m concat(objs\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\u001b[43mpa_read_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_rows\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult_files\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# by now only json files are expected as a single file\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     result \u001b[38;5;241m=\u001b[39m read_json(result_files[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/studium/masterarbeit/thesis-schneg/src/thesis_schneg/vis_modules.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m         result \u001b[38;5;241m=\u001b[39m pd_read_parquet(result_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         result \u001b[38;5;241m=\u001b[39m concat(objs\u001b[38;5;241m=\u001b[39m[\u001b[43mpa_read_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_rows\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     77\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m result_files], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# by now only json files are expected as a single file\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     result \u001b[38;5;241m=\u001b[39m read_json(result_files[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/studium/masterarbeit/thesis-schneg/src/venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:1762\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1762\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/studium/masterarbeit/thesis-schneg/src/venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:1328\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1326\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[0;32m-> 1328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileSystemDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfragment\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphysical_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilesystem\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pathlib import Path\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "    if dataset is None:\n",
    "        result_data = {}\n",
    "        # crawl files from all datasets and load into dictionary\n",
    "        paths = {f\"{name}\": _get_results_paths(name, analysis_name, cleaned_aql=take_new_directory) for name in [\n",
    "            \"aol\", \"aql\", \"ms-marco\", \"orcas\"]}\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            paths[\"aol\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        if analysis_name == 'character-count-frequencies-queries':\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-char-count-special\")\n",
    "            paths[\"aql\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "            \n",
    "        # iterate over datasets and create visualization\n",
    "        for name, result_paths in paths.items():\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading data from {name} dataset\")\n",
    "            vis_data = load_results(result_paths, test_data=test_data)\n",
    "            result_data.update({name: vis_data})\n",
    "            end_time = time.time()  \n",
    "            print(f\"{name} loaded in {(end_time - start_time)/60} min\")\n",
    "        color = ['blue', 'orange', 'red', 'purple']\n",
    "        label=[\"AOL\", \"AQL\", \"MS-MARCO\", \"ORCAS\"]\n",
    "        analysis_data.append(result_data)\n",
    "    else:\n",
    "        # load data from single dataset\n",
    "        result_paths = _get_results_paths(dataset, analysis_name, cleaned_aql=take_new_directory)\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            result_paths = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from {dataset} dataset\")\n",
    "        result_data = {dataset: load_results(result_paths)}\n",
    "        end_time = time.time()  \n",
    "        print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "        analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## for character-count-frequencies-words + entity-count-frequencies-queries\n",
    "analyses_params = []\n",
    "vis_params = {\"dataset-col-x\": \"character-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\",\n",
    "              \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Character per Word\"}\n",
    "analyses_params.append(vis_params)\n",
    "vis_params = {\"dataset-col-x\": \"entity-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\",\n",
    "              \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Named Entity per Query\"}\n",
    "analyses_params.append(vis_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing word-count-frequencies-named-entities\n",
      "[{'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [0, 10], 'y-lim': None, 'title': 'Words per Query'}, {'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [0, 10], 'y-lim': None, 'title': 'Words per Named Entity'}]\n",
      "Visualizing word-count-frequencies-named-entities\n",
      "[{'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [-0.5, 7.25], 'y-lim': None, 'title': 'Words per Query'}, {'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [0, 10], 'y-lim': None, 'title': 'Words per Named Entity'}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import bar_plot, get_xlim\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "analyses_params = []\n",
    "\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        if struc == 'queries':\n",
    "            sub_title = \"Query\"\n",
    "        elif struc == 'named-entities':\n",
    "            sub_title = \"Named Entity\"\n",
    "        elif struc == 'words':\n",
    "            sub_title = \"Word\"\n",
    "\n",
    "        # load visualization parametes into dictionary\n",
    "        if f\"{item}-{struc}\" == f'character-count-frequencies-{struc}':\n",
    "            vis_params = {\"dataset-col-x\": \"character-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\", \"y-label\": \"Frequency\", \"x-lim\": [0,10], \"y-lim\": None, \"title\": f\"Characters per {sub_title}\"}\n",
    "        elif f\"{item}-{struc}\" == f'word-count-frequencies-{struc}':\n",
    "            vis_params = {\"dataset-col-x\": \"word-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\", \"y-label\": \"Frequency\", \"x-lim\": [0,10], \"y-lim\": None, \"title\": f\"Words per {sub_title}\"}\n",
    "        elif f\"{item}-{struc}\" == f'entity-count-frequencies-{struc}':\n",
    "            vis_params = {\"dataset-col-x\": \"entity-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\", \"y-label\": \"Frequency\", \"x-lim\": [0,10], \"y-lim\": None, \"title\": f\"Named Entities per {sub_title}\"}\n",
    "        analyses_params.append(vis_params)\n",
    "\n",
    "\n",
    "# latex rendering for matplotlib\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "if take_new_directory:\n",
    "    base_path = base_path + \"updated_plots/\"\n",
    "# set path to save visualization\n",
    "if len(analyses) > 1:\n",
    "    vis_dir = Path(\n",
    "        f\"{base_path}{analyses[0]}-and-{analyses[1]}\")\n",
    "else:\n",
    "    vis_dir = Path(\n",
    "        f\"{base_path}{analyses[0]}\")\n",
    "    \n",
    "# vis_dir = Path(\n",
    "#         f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/extract-named-entities\")\n",
    "\n",
    "# make directory if it does not exist\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "# enable pgf format for matplotlib\n",
    "if save_vis:\n",
    "    matplotlib.use(\"pgf\")\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "# create subplots for each dataset\n",
    "# set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "\n",
    "bar_width = 0.5 \n",
    "if dataset is None:\n",
    "    fig, ax = plt.subplots(ncols=len(analyses), nrows=4, figsize=(width, 2*height-0.2*height))\n",
    "else:\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "for i in range(len(analyses)):   \n",
    "    result_data = analysis_data[i]\n",
    "    vis_params = analyses_params[i]\n",
    "    print(f\"Visualizing {analysis_name}\")\n",
    "    print(analyses_params)\n",
    "    # create visualization for all data sets if dataset is not specified\n",
    "    if dataset is None:\n",
    "\n",
    "        multi = True\n",
    "        # color palette for datasets\n",
    "        color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "\n",
    "        # get xlim\n",
    "        xlims = []\n",
    "        for name, vis_data in result_data.items():\n",
    "            xlims.append(get_xlim(vis_data, vis_params, threshold=0.054, bar_width=bar_width)[1]) #0.054\n",
    "        if max(xlims) > 10:\n",
    "            vis_params[\"x-lim\"] = [-1, max(xlims)]\n",
    "        else:\n",
    "            vis_params[\"x-lim\"] = [0-bar_width, max(xlims)]\n",
    "        # counter for datasets\n",
    "        cnt_datasets = 0\n",
    "        # iterate over datasets and create visualization\n",
    "        for name, vis_data in result_data.items():\n",
    "            # normalize data\n",
    "            if normalize_data:\n",
    "                vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                    vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "            vis_data = vis_data.sort_values(vis_params[\"dataset-col-x\"], ascending=True)\n",
    "            # apply specific visualization function\n",
    "            if len(analyses) > 1:\n",
    "                fig, ax[cnt_datasets,i] = bar_plot(data=vis_data, subplots=(fig, ax[cnt_datasets,i]),\n",
    "                                                vis_params=vis_params, label=label[cnt_datasets], multi=multi, color=color[cnt_datasets], bar_width=bar_width)\n",
    "                if cnt_datasets == 0:\n",
    "                    ax[cnt_datasets,i].set_title(vis_params[\"title\"])\n",
    "            else:\n",
    "                fig, ax[cnt_datasets] = bar_plot(data=vis_data, subplots=(fig, ax[cnt_datasets]),\n",
    "                                                vis_params=vis_params, label=name.upper(), multi=multi, color=color[cnt_datasets], bar_width=bar_width) #, color=color[cnt_datasets]\n",
    "                if cnt_datasets == 0:\n",
    "                    ax[cnt_datasets].set_title(vis_params[\"title\"])\n",
    "            cnt_datasets += 1\n",
    "            \n",
    "\n",
    "\n",
    "    # create visualization for a specific data set:\n",
    "    else: \n",
    "        multi = False \n",
    "        # get xlim\n",
    "        xlims = []\n",
    "        for name, vis_data in result_data.items():\n",
    "            xlims.append(get_xlim(vis_data, vis_params, threshold=0.015, bar_width=bar_width)[1]) #0.054\n",
    "        vis_params[\"x-lim\"] = [0-bar_width, max(xlims)]\n",
    "\n",
    "        if normalize_data:\n",
    "            vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                    vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "        \n",
    "        vis_data = vis_data.sort_values(vis_params[\"dataset-col-x\"], ascending=True)\n",
    "        \n",
    "        # modify title\n",
    "        vis_params[\"title\"] = f'{vis_params[\"title\"]} ({dataset.upper()})'\n",
    "\n",
    "        # set size of plot\n",
    "        textwidth = 5.5129\n",
    "        aspect_ratio = 6/8\n",
    "        scale = 1.0\n",
    "        width = textwidth * scale\n",
    "        height = width * aspect_ratio\n",
    "\n",
    "        # # create subplot for dataset\n",
    "        # fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "\n",
    "        # apply specific visualization function\n",
    "        fig, ax = bar_plot(data=vis_data, subplots=(fig, ax),\n",
    "                                                vis_params=vis_params, multi=multi,  bar_width=bar_width)\n",
    "        \n",
    "        # # make layout tight\n",
    "        # plt.tight_layout()\n",
    "        \n",
    "        # # either save visualization or show it\n",
    "        # if save_vis:\n",
    "        #     fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "        # else:\n",
    "        #     plt.show()\n",
    "\n",
    "if dataset is None:\n",
    "    fig.supxlabel(analyses_params[0][\"x-label\"], ha='center', va='center', y=0.033)\n",
    "    fig.supylabel(analyses_params[0][\"y-label\"])\n",
    "# make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# either save visualization or show it  \n",
    "if save_vis:\n",
    "    if dataset is not None:\n",
    "        if analyses[0] == 'character-count-frequencies-words' and dataset == 'aol' and cleaned_aol:\n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}-domains-cleaned.pgf\"))\n",
    "        else: \n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "    else:\n",
    "        fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Characters per Word* we can observe an anomaly at the distribution of the AOL log. Words of lengths 10 - 20 appear to be more frequent than in the other logs. Let's try to replicate this anomaly in the AQL log. For this, we filter the AQL by the timestamp of the queries. We only take words from queries of the year 2006, i.e. the release year of the AOL log.\n",
    "\n",
    "In order to get the frequencies of word lengths in 2006 we need to run the following commands of our CLI:\n",
    "1. `ray job submit --runtime-env ray-runtime-env.yml --no-wait -- python -m thesis_schneg analyser --dataset aql --analysis aql-get-words-2006 --read-concurrency 100 --concurrency 32 --write-concurrency 100 --memory-scaler 8 --batch-size 1024 `\n",
    "\n",
    "2. `ray job submit --runtime-env ray-runtime-env.yml --no-wait -- python -m thesis_schneg analyser --dataset aql --analysis get-lengths --struc-level words --read-concurrency 100 --concurrency 32 --write-concurrency 100 --memory-scaler 8 --batch-size 1024 --read-dir /mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-aql-get-words-2006-all`\n",
    "\n",
    "3. `ray job submit --runtime-env ray-runtime-env.yml --no-wait -- python -m thesis_schneg analyser --dataset aql --analysis character-count-frequencies --read-concurrency 100 --concurrency 32 --write-concurrency 100 --memory-scaler 8 --batch-size 1024 --read-dir /mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-lengths-words-special`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "# save visualization or not\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "dataset: DatasetName = 'aql'\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "base_analyses.append('character-count-frequencies') #-{struc}\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        analyses.append(f'{item}-{struc}')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pathlib import Path\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "\n",
    "\n",
    "    base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-special\")\n",
    "    result_paths = [path for path in base_path.iterdir(\n",
    "    )]\n",
    "    start_time = time.time()\n",
    "    print(f\"Loading data from {dataset} dataset\")\n",
    "    result_data = {dataset: load_results(result_paths)}\n",
    "    end_time = time.time()  \n",
    "    print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "    analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "analyses_params = []\n",
    "vis_params = {\"dataset-col-x\": \"character-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\",\n",
    "              \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Character per Word\"}\n",
    "analyses_params.append(vis_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import bar_plot, get_xlim\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "# latex rendering for matplotlib\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# set path to save visualization\n",
    "\n",
    "# vis_dir = Path(\n",
    "#     f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/aql-{analyses[0]}-2006\")\n",
    "\n",
    "vis_dir = Path(\n",
    "    f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/aql-{analyses[0]}-old-years\")\n",
    "    \n",
    "# vis_dir = Path(\n",
    "#         f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/extract-named-entities\")\n",
    "\n",
    "# make directory if it does not exist\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "# enable pgf format for matplotlib\n",
    "if save_vis:\n",
    "    matplotlib.use(\"pgf\")\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "# create subplots for each dataset\n",
    "# set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "\n",
    "bar_width = 0.5 \n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "for i in range(len(analyses)):   \n",
    "    result_data = analysis_data[i]\n",
    "    vis_params = analyses_params[i]\n",
    "    print(f\"Visualizing {analysis_name}\")\n",
    "    print(analyses_params)\n",
    "    # create visualization for all data sets if dataset is not specified\n",
    "\n",
    "\n",
    "\n",
    "    # create visualization for a specific data set:\n",
    "    multi = False \n",
    "    # get xlim\n",
    "    xlims = []\n",
    "    for name, vis_data in result_data.items():\n",
    "        xlims.append(get_xlim(vis_data, vis_params, threshold=0.01, bar_width=bar_width)[1]) #0.054\n",
    "    vis_params[\"x-lim\"] = [0-bar_width, max(xlims)]\n",
    "\n",
    "    if normalize_data:\n",
    "        vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "    \n",
    "    vis_data = vis_data.sort_values(vis_params[\"dataset-col-x\"], ascending=True)\n",
    "    \n",
    "    # modify title\n",
    "    vis_params[\"title\"] = f'{vis_params[\"title\"]} ({dataset.upper()})'\n",
    "\n",
    "    # set size of plot\n",
    "    textwidth = 5.5129\n",
    "    aspect_ratio = 6/8\n",
    "    scale = 1.0\n",
    "    width = textwidth * scale\n",
    "    height = width * aspect_ratio\n",
    "\n",
    "    # # create subplot for dataset\n",
    "    # fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "\n",
    "    # apply specific visualization function\n",
    "    fig, ax = bar_plot(data=vis_data, subplots=(fig, ax),\n",
    "                                            vis_params=vis_params, multi=multi,  bar_width=bar_width)\n",
    "    \n",
    "    # # make layout tight\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    # # either save visualization or show it\n",
    "    # if save_vis:\n",
    "    #     fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "    # else:\n",
    "    #     plt.show()\n",
    "\n",
    "if dataset is None:\n",
    "    fig.supxlabel(analyses_params[0][\"x-label\"])\n",
    "    fig.supylabel(analyses_params[0][\"y-label\"])\n",
    "# make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# either save visualization or show it  \n",
    "if save_vis:\n",
    "    if dataset is not None:\n",
    "        if analyses[0] == 'character-count-frequencies-words' and dataset == 'aol' and cleaned_aol:\n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}-domains-cleaned.pgf\"))\n",
    "        else: \n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "    else:\n",
    "        fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

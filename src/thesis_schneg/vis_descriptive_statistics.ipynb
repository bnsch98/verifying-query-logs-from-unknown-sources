{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Descriptive Statistics\n",
    "\n",
    "Of the 4 involved data sets a collection of statistics is created to gain a first insight.\n",
    "For this, we extract named entities, words and characters from the queries. Accordingly, 4 subcategories arise:\n",
    "- queries\n",
    "- named entities\n",
    "- words\n",
    "- characters \n",
    "\n",
    "For each category, the length is measured in terms of all sublevels. E.g. the length of a named entity can be described as the number of words or the number of characters. Alongside, the items of each subcategory are sorted with respect to their frequency to check for specific liguistic laws like zipfs's law. Essentially, we obtain a thorough set of statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Zipf's law \n",
    "Zipf's law states that frequencies of words from a document follow an inversely proportional distribution to their rank when sorted in descending order. To get a more comprehensive comparison between the data sets, we check on zipf's law for all subcategories.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, lets define some configuration for our visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "# combine all plots in one figure or plot each analysis in a single figure\n",
    "single_plot = True\n",
    "# single_plot = False\n",
    "\n",
    "# save visualization or not\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "# select dataset (if none is selected all datasets are visualized in a joint plot) \n",
    "dataset: DatasetName = None \n",
    "# dataset: DatasetName = 'aol'\n",
    "# dataset: DatasetName = 'aql'\n",
    "# dataset: DatasetName = 'ms-marco'\n",
    "# dataset: DatasetName = 'orcas'\n",
    "\n",
    "analyses = []\n",
    "# set analysis that should be visualized\n",
    "# analyses.append('query-frequencies')\n",
    "analyses.append('extract-named-entities')\n",
    "analyses.append('extract-words')\n",
    "# analyses.append('extract-chars')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False\n",
    "\n",
    "# take_new_directory = True\n",
    "take_new_directory = False\n",
    "\n",
    "col = ['count()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the specified data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from aol dataset\n",
      "aol loaded in 5.158525168895721 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 7.6204802592595415 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 5.650026520093282 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 4.2598038673400875 min\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.38040762742360434 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 8.743824430306752 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.3950100262959798 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.3515229860941569 min\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "    if dataset is None:\n",
    "        result_data = {}\n",
    "        # crawl files from all datasets and load into dictionary\n",
    "        if analysis_name == 'query-frequencies':\n",
    "            paths = {f\"{name}\": _get_results_paths(name, analysis_name, take_new_directory) for name in [\n",
    "                \"aol\", \"aql\"]}\n",
    "        else:\n",
    "            paths = {f\"{name}\": _get_results_paths(name, analysis_name, take_new_directory) for name in [\n",
    "                \"aol\", \"aql\", \"ms-marco\", \"orcas\"]}\n",
    "        # iterate over datasets and create visualization\n",
    "        for name, result_paths in paths.items():\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading data from {name} dataset\")\n",
    "            vis_data = load_results(result_paths, test_data=test_data, cols=col)\n",
    "            result_data.update({name: vis_data})\n",
    "            end_time = time.time()  \n",
    "            print(f\"{name} loaded in {(end_time - start_time)/60} min\")\n",
    "        label=[\"AOL\", \"AQL\", \"MS-MARCO\", \"ORCAS\"]\n",
    "        analysis_data.append(result_data)\n",
    "    else:\n",
    "        # load data from single dataset\n",
    "        result_paths = _get_results_paths(dataset, analysis_name, take_new_directory)\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from {dataset} dataset\")\n",
    "        result_data = {dataset: load_results(result_paths, test_data=test_data, cols=col)}\n",
    "        end_time = time.time()  \n",
    "        print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "        analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing extract-named-entities\n",
      "{'dataset-col-x': 'entity', 'dataset-col-y': 'count()', 'x-label': 'Rank', 'y-label': 'Frequency', 'x-lim': None, 'y-lim': None, 'title': \"Zipf's Law Named Entities\"}\n",
      "Visualizing extract-words\n",
      "{'dataset-col-x': 'word', 'dataset-col-y': 'count()', 'x-label': 'Rank', 'y-label': 'Frequency', 'x-lim': None, 'y-lim': None, 'title': \"Zipf's Law Words\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5586/167508293.py:151: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5586/167508293.py:155: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization finished\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import log_plot\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "analyses_params = []\n",
    "for analysis_name in analyses:\n",
    "    # load visualization parametes into dictionary\n",
    "    if analysis_name == 'query-frequencies':\n",
    "        vis_params = {\"dataset-col-x\": \"serp_query_text_url\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Queries\"}\n",
    "    elif analysis_name == 'extract-named-entities':\n",
    "        vis_params = {\"dataset-col-x\": \"entity\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Named Entities\"}\n",
    "    elif analysis_name == 'extract-words':\n",
    "        vis_params = {\"dataset-col-x\": \"word\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Words\"}\n",
    "    elif analysis_name == 'extract-chars':\n",
    "        vis_params = {\"dataset-col-x\": \"char\", \"dataset-col-y\": \"count()\", \"x-label\": \"Rank\",\n",
    "                      \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Zipf's Law Characters\"}\n",
    "    analyses_params.append(vis_params)\n",
    "\n",
    "# latex rendering for matplotlib\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "if take_new_directory:\n",
    "    base_path = base_path + \"updated_plots/\"\n",
    "\n",
    "# set path to save visualization\n",
    "if len(analyses) > 1:\n",
    "    if single_plot:\n",
    "        vis_dir = Path(\n",
    "            f\"{base_path}{analyses[0]}-and-{analyses[1]}-single\")\n",
    "    else:\n",
    "        vis_dir = Path(\n",
    "            f\"{base_path}{analyses[0]}-and-{analyses[1]}\")\n",
    "else:\n",
    "    if single_plot:\n",
    "        vis_dir = Path(\n",
    "            f\"{base_path}{analyses[0]}-single\")\n",
    "    else:\n",
    "        vis_dir = Path(\n",
    "            f\"{base_path}{analyses[0]}\")\n",
    "    \n",
    "# vis_dir = Path(\n",
    "#         f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/extract-named-entities\")\n",
    "\n",
    "# make directory if it does not exist\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "# enable pgf format for matplotlib\n",
    "if save_vis:\n",
    "    matplotlib.use(\"pgf\")\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "# create subplots for each dataset\n",
    "# set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "if single_plot:\n",
    "    fig, ax = plt.subplots(ncols=len(analyses), nrows=1,\n",
    "                           figsize=(width*1.03, 4*height/5))\n",
    "else:\n",
    "    fig, ax = plt.subplots(ncols=len(analyses), nrows=4,\n",
    "                           figsize=(width, 2*height))\n",
    "\n",
    "for i in range(len(analyses)):\n",
    "    result_data = analysis_data[i]\n",
    "    vis_params = analyses_params[i]\n",
    "    print(f\"Visualizing {analyses[i]}\")\n",
    "    print(vis_params)\n",
    "    # create visualization for all data sets if dataset is not specified\n",
    "    if dataset is None:\n",
    "\n",
    "        multi = True\n",
    "        # color palette for datasets\n",
    "        color = ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "        linestyles = [\"solid\", \"dashdot\", \"dashed\", \"dotted\"]\n",
    "        # counter for datasets\n",
    "        cnt_datasets = 0\n",
    "        # iterate over datasets and create visualization\n",
    "        for name, vis_data in result_data.items():\n",
    "            if normalize_data:\n",
    "                vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                    vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "            # apply specific visualization function\n",
    "            if not single_plot:\n",
    "                if len(analyses) > 1:\n",
    "                    fig, ax[cnt_datasets, i] = log_plot(data=vis_data, subplots=(fig, ax[cnt_datasets, i]),\n",
    "                                                        vis_params=vis_params, label=label[cnt_datasets], linestyle=linestyles[cnt_datasets], multi=multi, color=color[cnt_datasets])\n",
    "                else:\n",
    "                    fig, ax[cnt_datasets] = log_plot(data=vis_data, subplots=(fig, ax[cnt_datasets]),\n",
    "                                                     vis_params=vis_params, label=name.upper(), multi=multi, linestyle=linestyles[cnt_datasets], color=color[cnt_datasets])  # , color=color[cnt_datasets]\n",
    "            else:\n",
    "                if len(analyses) > 1:\n",
    "                    fig, ax[i] = log_plot(data=vis_data, subplots=(fig, ax[i]),\n",
    "                                          vis_params=vis_params, label=label[cnt_datasets], multi=multi, linestyle=linestyles[cnt_datasets], color=color[cnt_datasets])\n",
    "                else:\n",
    "                    fig, ax = log_plot(data=vis_data, subplots=(fig, ax),\n",
    "                                       vis_params=vis_params, label=name.upper(), multi=multi, linestyle=linestyles[cnt_datasets], color=color[cnt_datasets])  # , color=color[cnt_datasets]\n",
    "            cnt_datasets += 1\n",
    "\n",
    "    # create visualization for a specific data set:\n",
    "    else:\n",
    "        multi = False\n",
    "\n",
    "        # modify title\n",
    "        vis_params[\"title\"] = f'{vis_params[\"title\"]} ({dataset.upper()})'\n",
    "\n",
    "        # set size of plot\n",
    "        textwidth = 5.5129\n",
    "        aspect_ratio = 6/8\n",
    "        scale = 1.0\n",
    "        width = textwidth * scale\n",
    "        height = width * aspect_ratio\n",
    "\n",
    "        # create subplot for dataset\n",
    "        fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "\n",
    "        # apply specific visualization function\n",
    "        fig, ax = log_plot(data=result_data[dataset], subplots=(fig, ax),\n",
    "                           vis_params=vis_params, multi=multi)\n",
    "\n",
    "        # make layout tight\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # either save visualization or show it\n",
    "        if save_vis:\n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if dataset is None:\n",
    "    fig.supxlabel(analyses_params[0][\"x-label\"], y=0.05)\n",
    "    fig.supylabel(analyses_params[0][\"y-label\"])\n",
    "# make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# either save visualization or show it\n",
    "if save_vis:\n",
    "    fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Length Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the lengths of queries and their previously defined subcategories. For the sake of clarity we plot the results of each data set seperately.\n",
    "\n",
    "Visualization configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "# save visualization or not\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "# select dataset (if none is selected all datasets are visualized in a joint plot) \n",
    "dataset: DatasetName = None \n",
    "# dataset: DatasetName = 'aol'\n",
    "# dataset: DatasetName = 'aql'\n",
    "# dataset: DatasetName = 'ms-marco'\n",
    "# dataset: DatasetName = 'orcas'\n",
    "\n",
    "# plot cleaned aol data or not\n",
    "cleaned_aol = False\n",
    "# cleaned_aol = True\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('queries')\n",
    "struc_level.append('named-entities')\n",
    "# struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "# base_analyses.append('character-count-frequencies') #-{struc}\n",
    "base_analyses.append('word-count-frequencies')\n",
    "# base_analyses.append('entity-count-frequencies')\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        analyses.append(f'{item}-{struc}')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False\n",
    "\n",
    "take_new_directory = True #takes new directory for plots and takes cleaned aql\n",
    "# take_new_directory = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "analyses = []\n",
    "analyses.append('character-count-frequencies-words') #-{struc}\n",
    "analyses.append('entity-count-frequencies-queries') #-{struc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-word-count-frequencies-queries-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.00019446611404418945 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.1394240975379944 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.00020220279693603516 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.00011528730392456055 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-word-count-frequencies-named-entities-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 9.756088256835937e-05 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.05131059487660726 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 9.905099868774414e-05 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 8.69433085123698e-05 min\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pathlib import Path\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "    if dataset is None:\n",
    "        result_data = {}\n",
    "        # crawl files from all datasets and load into dictionary\n",
    "        paths = {f\"{name}\": _get_results_paths(name, analysis_name, cleaned_aql=take_new_directory) for name in [\n",
    "            \"aol\", \"aql\", \"ms-marco\", \"orcas\"]}\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            paths[\"aol\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        if analysis_name == 'character-count-frequencies-queries':\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-char-count-special\")\n",
    "            paths[\"aql\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "            \n",
    "        # iterate over datasets and create visualization\n",
    "        for name, result_paths in paths.items():\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading data from {name} dataset\")\n",
    "            vis_data = load_results(result_paths, test_data=test_data)\n",
    "            result_data.update({name: vis_data})\n",
    "            end_time = time.time()  \n",
    "            print(f\"{name} loaded in {(end_time - start_time)/60} min\")\n",
    "        color = ['blue', 'orange', 'red', 'purple']\n",
    "        label=[\"AOL\", \"AQL\", \"MS-MARCO\", \"ORCAS\"]\n",
    "        analysis_data.append(result_data)\n",
    "    else:\n",
    "        # load data from single dataset\n",
    "        result_paths = _get_results_paths(dataset, analysis_name, cleaned_aql=take_new_directory)\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            result_paths = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from {dataset} dataset\")\n",
    "        result_data = {dataset: load_results(result_paths)}\n",
    "        end_time = time.time()  \n",
    "        print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "        analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## for character-count-frequencies-words + entity-count-frequencies-queries\n",
    "analyses_params = []\n",
    "vis_params = {\"dataset-col-x\": \"character-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\",\n",
    "              \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Character per Word\"}\n",
    "analyses_params.append(vis_params)\n",
    "vis_params = {\"dataset-col-x\": \"entity-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\",\n",
    "              \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Named Entity per Query\"}\n",
    "analyses_params.append(vis_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing word-count-frequencies-named-entities\n",
      "[{'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [0, 10], 'y-lim': None, 'title': 'Words per Query'}, {'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [0, 10], 'y-lim': None, 'title': 'Words per Named Entity'}]\n",
      "Visualizing word-count-frequencies-named-entities\n",
      "[{'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [-0.5, 7.25], 'y-lim': None, 'title': 'Words per Query'}, {'dataset-col-x': 'word-count', 'dataset-col-y': 'count()', 'x-label': 'Count', 'y-label': 'Frequency', 'x-lim': [0, 10], 'y-lim': None, 'title': 'Words per Named Entity'}]\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import bar_plot, get_xlim\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "analyses_params = []\n",
    "\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        if struc == 'queries':\n",
    "            sub_title = \"Query\"\n",
    "        elif struc == 'named-entities':\n",
    "            sub_title = \"Named Entity\"\n",
    "        elif struc == 'words':\n",
    "            sub_title = \"Word\"\n",
    "\n",
    "        # load visualization parametes into dictionary\n",
    "        if f\"{item}-{struc}\" == f'character-count-frequencies-{struc}':\n",
    "            vis_params = {\"dataset-col-x\": \"character-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\", \"y-label\": \"Frequency\", \"x-lim\": [0,10], \"y-lim\": None, \"title\": f\"Characters per {sub_title}\"}\n",
    "        elif f\"{item}-{struc}\" == f'word-count-frequencies-{struc}':\n",
    "            vis_params = {\"dataset-col-x\": \"word-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\", \"y-label\": \"Frequency\", \"x-lim\": [0,10], \"y-lim\": None, \"title\": f\"Words per {sub_title}\"}\n",
    "        elif f\"{item}-{struc}\" == f'entity-count-frequencies-{struc}':\n",
    "            vis_params = {\"dataset-col-x\": \"entity-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\", \"y-label\": \"Frequency\", \"x-lim\": [0,10], \"y-lim\": None, \"title\": f\"Named Entities per {sub_title}\"}\n",
    "        analyses_params.append(vis_params)\n",
    "\n",
    "\n",
    "# latex rendering for matplotlib\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "if take_new_directory:\n",
    "    base_path = base_path + \"updated_plots/\"\n",
    "# set path to save visualization\n",
    "if len(analyses) > 1:\n",
    "    vis_dir = Path(\n",
    "        f\"{base_path}{analyses[0]}-and-{analyses[1]}\")\n",
    "else:\n",
    "    vis_dir = Path(\n",
    "        f\"{base_path}{analyses[0]}\")\n",
    "    \n",
    "# vis_dir = Path(\n",
    "#         f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/extract-named-entities\")\n",
    "\n",
    "# make directory if it does not exist\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "# enable pgf format for matplotlib\n",
    "if save_vis:\n",
    "    matplotlib.use(\"pgf\")\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "# create subplots for each dataset\n",
    "# set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "\n",
    "bar_width = 0.5 \n",
    "if dataset is None:\n",
    "    fig, ax = plt.subplots(ncols=len(analyses), nrows=4, figsize=(width, 2*height-0.1*height))\n",
    "else:\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "for i in range(len(analyses)):   \n",
    "    result_data = analysis_data[i]\n",
    "    vis_params = analyses_params[i]\n",
    "    print(f\"Visualizing {analysis_name}\")\n",
    "    print(analyses_params)\n",
    "    # create visualization for all data sets if dataset is not specified\n",
    "    if dataset is None:\n",
    "\n",
    "        multi = True\n",
    "        # color palette for datasets\n",
    "        color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "\n",
    "        # get xlim\n",
    "        xlims = []\n",
    "        for name, vis_data in result_data.items():\n",
    "            xlims.append(get_xlim(vis_data, vis_params, threshold=0.054, bar_width=bar_width)[1]) #0.054\n",
    "        if max(xlims) > 10:\n",
    "            vis_params[\"x-lim\"] = [-1, max(xlims)]\n",
    "        else:\n",
    "            vis_params[\"x-lim\"] = [0-bar_width, max(xlims)]\n",
    "        # counter for datasets\n",
    "        cnt_datasets = 0\n",
    "        # iterate over datasets and create visualization\n",
    "        for name, vis_data in result_data.items():\n",
    "            # normalize data\n",
    "            if normalize_data:\n",
    "                vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                    vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "            vis_data = vis_data.sort_values(vis_params[\"dataset-col-x\"], ascending=True)\n",
    "            # apply specific visualization function\n",
    "            if len(analyses) > 1:\n",
    "                fig, ax[cnt_datasets,i] = bar_plot(data=vis_data, subplots=(fig, ax[cnt_datasets,i]),\n",
    "                                                vis_params=vis_params, label=label[cnt_datasets], multi=multi, color=color[cnt_datasets], bar_width=bar_width)\n",
    "                if cnt_datasets == 0:\n",
    "                    ax[cnt_datasets,i].set_title(vis_params[\"title\"])\n",
    "            else:\n",
    "                fig, ax[cnt_datasets] = bar_plot(data=vis_data, subplots=(fig, ax[cnt_datasets]),\n",
    "                                                vis_params=vis_params, label=name.upper(), multi=multi, color=color[cnt_datasets], bar_width=bar_width) #, color=color[cnt_datasets]\n",
    "                if cnt_datasets == 0:\n",
    "                    ax[cnt_datasets].set_title(vis_params[\"title\"])\n",
    "            cnt_datasets += 1\n",
    "            \n",
    "\n",
    "\n",
    "    # create visualization for a specific data set:\n",
    "    else: \n",
    "        multi = False \n",
    "        # get xlim\n",
    "        xlims = []\n",
    "        for name, vis_data in result_data.items():\n",
    "            xlims.append(get_xlim(vis_data, vis_params, threshold=0.015, bar_width=bar_width)[1]) #0.054\n",
    "        vis_params[\"x-lim\"] = [0-bar_width, max(xlims)]\n",
    "\n",
    "        if normalize_data:\n",
    "            vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                    vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "        \n",
    "        vis_data = vis_data.sort_values(vis_params[\"dataset-col-x\"], ascending=True)\n",
    "        \n",
    "        # modify title\n",
    "        vis_params[\"title\"] = f'{vis_params[\"title\"]} ({dataset.upper()})'\n",
    "\n",
    "        # set size of plot\n",
    "        textwidth = 5.5129\n",
    "        aspect_ratio = 6/8\n",
    "        scale = 1.0\n",
    "        width = textwidth * scale\n",
    "        height = width * aspect_ratio\n",
    "\n",
    "        # # create subplot for dataset\n",
    "        # fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "\n",
    "        # apply specific visualization function\n",
    "        fig, ax = bar_plot(data=vis_data, subplots=(fig, ax),\n",
    "                                                vis_params=vis_params, multi=multi,  bar_width=bar_width)\n",
    "        \n",
    "        # # make layout tight\n",
    "        # plt.tight_layout()\n",
    "        \n",
    "        # # either save visualization or show it\n",
    "        # if save_vis:\n",
    "        #     fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "        # else:\n",
    "        #     plt.show()\n",
    "\n",
    "if dataset is None:\n",
    "    fig.supxlabel(analyses_params[0][\"x-label\"], ha='center', va='center', y=0.033)\n",
    "    fig.supylabel(analyses_params[0][\"y-label\"])\n",
    "# make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# either save visualization or show it  \n",
    "if save_vis:\n",
    "    if dataset is not None:\n",
    "        if analyses[0] == 'character-count-frequencies-words' and dataset == 'aol' and cleaned_aol:\n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}-domains-cleaned.pgf\"))\n",
    "        else: \n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "    else:\n",
    "        fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Characters per Word* we can observe an anomaly at the distribution of the AOL log. Words of lengths 10 - 20 appear to be more frequent than in the other logs. Let's try to replicate this anomaly in the AQL log. For this, we filter the AQL by the timestamp of the queries. We only take words from queries of the year 2006, i.e. the release year of the AOL log.\n",
    "\n",
    "In order to get the frequencies of word lengths in 2006 we need to run the following commands of our CLI:\n",
    "1. `ray job submit --runtime-env ray-runtime-env.yml --no-wait -- python -m thesis_schneg analyser --dataset aql --analysis aql-get-words-2006 --read-concurrency 100 --concurrency 32 --write-concurrency 100 --memory-scaler 8 --batch-size 1024 `\n",
    "\n",
    "2. `ray job submit --runtime-env ray-runtime-env.yml --no-wait -- python -m thesis_schneg analyser --dataset aql --analysis get-lengths --struc-level words --read-concurrency 100 --concurrency 32 --write-concurrency 100 --memory-scaler 8 --batch-size 1024 --read-dir /mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-aql-get-words-2006-all`\n",
    "\n",
    "3. `ray job submit --runtime-env ray-runtime-env.yml --no-wait -- python -m thesis_schneg analyser --dataset aql --analysis character-count-frequencies --read-concurrency 100 --concurrency 32 --write-concurrency 100 --memory-scaler 8 --batch-size 1024 --read-dir /mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-lengths-words-special`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "# save visualization or not\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "dataset: DatasetName = 'aql'\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "base_analyses.append('character-count-frequencies') #-{struc}\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        analyses.append(f'{item}-{struc}')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pathlib import Path\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "\n",
    "\n",
    "    base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-special\")\n",
    "    result_paths = [path for path in base_path.iterdir(\n",
    "    )]\n",
    "    start_time = time.time()\n",
    "    print(f\"Loading data from {dataset} dataset\")\n",
    "    result_data = {dataset: load_results(result_paths)}\n",
    "    end_time = time.time()  \n",
    "    print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "    analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "analyses_params = []\n",
    "vis_params = {\"dataset-col-x\": \"character-count\", \"dataset-col-y\": \"count()\", \"x-label\": \"Count\",\n",
    "              \"y-label\": \"Frequency\", \"x-lim\": None, \"y-lim\": None, \"title\": \"Character per Word\"}\n",
    "analyses_params.append(vis_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import bar_plot, get_xlim\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "# latex rendering for matplotlib\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# set path to save visualization\n",
    "\n",
    "# vis_dir = Path(\n",
    "#     f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/aql-{analyses[0]}-2006\")\n",
    "\n",
    "vis_dir = Path(\n",
    "    f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/aql-{analyses[0]}-old-years\")\n",
    "    \n",
    "# vis_dir = Path(\n",
    "#         f\"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/extract-named-entities\")\n",
    "\n",
    "# make directory if it does not exist\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "# enable pgf format for matplotlib\n",
    "if save_vis:\n",
    "    matplotlib.use(\"pgf\")\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "# create subplots for each dataset\n",
    "# set size of plot\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "\n",
    "bar_width = 0.5 \n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "for i in range(len(analyses)):   \n",
    "    result_data = analysis_data[i]\n",
    "    vis_params = analyses_params[i]\n",
    "    print(f\"Visualizing {analysis_name}\")\n",
    "    print(analyses_params)\n",
    "    # create visualization for all data sets if dataset is not specified\n",
    "\n",
    "\n",
    "\n",
    "    # create visualization for a specific data set:\n",
    "    multi = False \n",
    "    # get xlim\n",
    "    xlims = []\n",
    "    for name, vis_data in result_data.items():\n",
    "        xlims.append(get_xlim(vis_data, vis_params, threshold=0.01, bar_width=bar_width)[1]) #0.054\n",
    "    vis_params[\"x-lim\"] = [0-bar_width, max(xlims)]\n",
    "\n",
    "    if normalize_data:\n",
    "        vis_data[vis_params[\"dataset-col-y\"]] = vis_data[vis_params[\"dataset-col-y\"]] / \\\n",
    "                vis_data[vis_params[\"dataset-col-y\"]].sum()\n",
    "    \n",
    "    vis_data = vis_data.sort_values(vis_params[\"dataset-col-x\"], ascending=True)\n",
    "    \n",
    "    # modify title\n",
    "    vis_params[\"title\"] = f'{vis_params[\"title\"]} ({dataset.upper()})'\n",
    "\n",
    "    # set size of plot\n",
    "    textwidth = 5.5129\n",
    "    aspect_ratio = 6/8\n",
    "    scale = 1.0\n",
    "    width = textwidth * scale\n",
    "    height = width * aspect_ratio\n",
    "\n",
    "    # # create subplot for dataset\n",
    "    # fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(width, height))\n",
    "\n",
    "    # apply specific visualization function\n",
    "    fig, ax = bar_plot(data=vis_data, subplots=(fig, ax),\n",
    "                                            vis_params=vis_params, multi=multi,  bar_width=bar_width)\n",
    "    \n",
    "    # # make layout tight\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    # # either save visualization or show it\n",
    "    # if save_vis:\n",
    "    #     fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "    # else:\n",
    "    #     plt.show()\n",
    "\n",
    "if dataset is None:\n",
    "    fig.supxlabel(analyses_params[0][\"x-label\"])\n",
    "    fig.supylabel(analyses_params[0][\"y-label\"])\n",
    "# make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# either save visualization or show it  \n",
    "if save_vis:\n",
    "    if dataset is not None:\n",
    "        if analyses[0] == 'character-count-frequencies-words' and dataset == 'aol' and cleaned_aol:\n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}-domains-cleaned.pgf\"))\n",
    "        else: \n",
    "            fig.savefig(vis_dir.joinpath(f\"{dataset}.pgf\"))\n",
    "    else:\n",
    "        fig.savefig(vis_dir.joinpath(\"all.pgf\"))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of the resulting distributions\n",
    "In this notebook we want to compare the resulting distributions by applying various statistical methods to quantify differences and similarities.\n",
    "## Frequencies of Linguistic Elements\n",
    "To assess the similarity of the resulting distributions, we compute the [*Wasserstein-Metric*](https://en.wikipedia.org/wiki/Wasserstein_metric) of each pair of distributions.\n",
    "The Wasserstein-Metric is a distance function for probability distributions. It satisfies the criteria of actual mathematical metrics and thus enables comparisons between probability distributions. Assuming one-dimensional distributions, the Wasserstein Distance of two empirical samples $P$ and $Q$ with respective random variables $X_1,...,X_n$ and $Y_1,...,Y_n$ is given by:\n",
    "$$ W_{p}(P,Q)=\\left({\\frac {1}{n}}\\sum _{i=1}^{n}\\|X_{(i)}-Y_{(i)}\\|^{p}\\right)^{\\frac {1}{p}}$$ \n",
    "Let's compute the Wasserstein-Metric for the frequencies of Characters, Words, Named Entities and Queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pyarrow import compute as pc\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "analyses = []\n",
    "# set analysis that should be analyzed\n",
    "# analyses.append('query-frequencies')\n",
    "# analyses.append('extract-named-entities')\n",
    "# analyses.append('extract-words')\n",
    "analyses.append('extract-chars')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "cleaned_aql = True\n",
    "\n",
    "col = ['count()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading \"extract-chars\"\n",
      "AOL loaded in 0.13485427697499594 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-extract-chars-special\n",
      "AQL loaded in 1.032812809944153 min\n",
      "MS-MARCO loaded in 1.0881295522054037 min\n",
      "ORCAS loaded in 0.10830211242039998 min\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# load data\n",
    "analysis_data = {}\n",
    "for analysis in analyses:\n",
    "    print(f\"Start loading \\\"{analysis}\\\"\")\n",
    "    datasets = {}\n",
    "    for dataset in [\"aol\",\"aql\",\"ms-marco\",\"orcas\"]:\n",
    "        start_time = time()\n",
    "        paths = _get_results_paths(dataset, analysis, cleaned_aql)\n",
    "        result_data = load_results(paths, test_data=test_data, cols=col)\n",
    "        datasets.update({dataset: result_data})\n",
    "        end_time = time()\n",
    "        print(f\"{dataset.upper()} loaded in {(end_time - start_time)/60} min\")\n",
    "    analysis_data.update({analysis: datasets})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Wasserstein-Metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract-chars\n",
      "get x_max...\n",
      "aol\n",
      "aql\n",
      "ms-marco\n",
      "orcas\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import get_max_x\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "cnt=0\n",
    "distances_data = {}\n",
    "for analysis, datasets in analysis_data.items():\n",
    "    print(analysis)\n",
    "    x_max = get_max_x(datasets, \"count()\")\n",
    "    # print(x_max)\n",
    "    # distances = ndarray((len(datasets), len(datasets)))\n",
    "    distances = DataFrame(np.zeros((len(datasets), len(datasets))), index=datasets.keys(), columns=datasets.keys())\n",
    "    names = []\n",
    "    j=0\n",
    "    for dataset_name, data in datasets.items():\n",
    "        data1 = {dataset_name: data.sort_values(\"count()\", ascending=True)}\n",
    "        names.append(dataset_name)\n",
    "        x_vals = np.arange(1, x_max+1)\n",
    "        y_vals = data1[dataset_name]['count()'][0:x_max].to_numpy()\n",
    "        if len(y_vals) < x_max:\n",
    "            y_vals = np.append(y_vals, np.zeros(x_max-len(y_vals)))\n",
    "        i=0\n",
    "        for dataset_name, data in datasets.items():\n",
    "            if dataset_name in names:\n",
    "                dist = 0\n",
    "            else:\n",
    "                data2 = data.sort_values(\"count()\", ascending=True)\n",
    "                x_vals2 = np.arange(1, x_max+1)\n",
    "                y_vals2 = data2['count()'][0:x_max].to_numpy()\n",
    "                if len(y_vals2) < x_max:\n",
    "                    y_vals2 = np.append(y_vals2, np.zeros(x_max-len(y_vals2)))\n",
    "                dist = wasserstein_distance(x_vals, x_vals2, u_weights=y_vals, v_weights=y_vals2)\n",
    "            # distances[i][j] = dist\n",
    "            distances.iloc[i, j] = dist\n",
    "            i+=1\n",
    "        j+=1\n",
    "    distances = distances + distances.T\n",
    "    distances_data.update({analysis: distances})\n",
    "    cnt+=1\n",
    "for key, value in distances_data.items():\n",
    "    print(value)\n",
    "\n",
    "## get avarage wasserstein distances per query log\n",
    "avg_distances = {}\n",
    "for analysis, distances in distances_data.items():\n",
    "    avg_distances.update({analysis: distances.mean().mean()})\n",
    "    \n",
    "for key, value in avg_distances.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Distance Configuration by applying Multidimensional Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from matplotlib import pyplot as plt, use\n",
    "from pathlib import Path    \n",
    "import scienceplots\n",
    "\n",
    "# save visualization or not\n",
    "# save_vis: bool = False\n",
    "save_vis: bool = True\n",
    "\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "save_vis = True\n",
    "# save_vis = False\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "trans_dists = {}\n",
    "color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "for key, value in distances_data.items():\n",
    "    print(key)\n",
    "    trans_dists.update({key: mds.fit_transform(value)})\n",
    "\n",
    "num_plots = len(trans_dists)\n",
    "cols = 3\n",
    "rows = num_plots//cols\n",
    "if num_plots % cols != 0:\n",
    "    rows+=1\n",
    "\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "\n",
    "# fig,ax = plt.subplots(rows,cols, figsize=(4*cols,4*rows))\n",
    "fig,ax = plt.subplots(rows,cols, figsize=(width,height))\n",
    "\n",
    "# for key, value in trans_dists.items() and ax in ax.flatten():\n",
    "key = [key for key in trans_dists.keys()]\n",
    "# title_list = []\n",
    "# for i in key:\n",
    "#     title_strings = {}\n",
    "#     if key.split('-')[0] == 'character':\n",
    "#         title_strings.update({key.split('-')[0]: [key]})\n",
    "axes = ax.flatten()\n",
    "for i in range(len(trans_dists)):\n",
    "    ax = axes[i]\n",
    "    value = trans_dists[key[i]]\n",
    "    # plt.figure()\n",
    "    for j in range(len(value)):\n",
    "        ax.scatter(value[j][0], value[j][1], c=color[j], label=names[j].upper())\n",
    "    # plt.scatter(trans_dists[key][:,0],trans_dists[key][:,1], c=color, label=names)\n",
    "    # ax.title(\"MDS Plot of Wasserstein Distances\")\n",
    "    # plt.legend()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"{key[i].title()}\")\n",
    "    # ax.yticks([])\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels,fancybox=True,framealpha=0.5).get_frame().set_linewidth(0.5)\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.01), fancybox=False, ncol=len(names),edgecolor=\"black\").get_frame().set_linewidth(0.5)\n",
    "\n",
    "# fig.suptitle(\"MDS Plot of Wasserstein Distances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "\n",
    "vis_dir = Path(f\"{base_path}Wasserstein-Distances-Linguistic-Elements\")\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "if save_vis:\n",
    "    use(\"pgf\")\n",
    "    fig.savefig(vis_dir.joinpath(\"all.pgf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length-related Frequencies\n",
    "### 1. Kolmogerov-Smirnov-Test\n",
    "First of all, we carry out a statistical test to check if the different distributions result from a common underlying distribution. We select the [*Kolmogerov-Smirnov-Test*](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test) to verify this hypothesis.  \n",
    "\n",
    "The test computes a test statistic $D$ that is compared to a threshold $D_{\\alpha}$ with a significance level $\\alpha$. If\n",
    "$$D < D_{\\alpha}$$ \n",
    "then the two distributions are likely to be similar. If, in contrast, \n",
    "$$D \\ge D_{\\alpha}$$ \n",
    "then we can conclude the samples are not from a common distribution with a significance of $1-\\alpha$. The test measures the maximum distance of the cumulative distribution functions $F$ of the involved distributions. Accordingly, $D$ is computed by\n",
    "$$D = \\mathrm{max} |( F_{1} - F_{2} )| $$\n",
    "The threshold can be determined by \n",
    "$$D_{\\alpha} = K_{\\alpha}\\sqrt{(n_1+n_2)/(n_1 \\cdot n_2)} $$\n",
    "$K_{\\alpha}$ is a constant that is dependent on the significance level $\\alpha$ and can be obtained by this table:\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "\n",
    "\n",
    "# select dataset (if none is selected all datasets are visualized in a joint plot) \n",
    "dataset: DatasetName = None \n",
    "# dataset: DatasetName = 'aol'\n",
    "# dataset: DatasetName = 'aql'\n",
    "# dataset: DatasetName = 'ms-marco'\n",
    "# dataset: DatasetName = 'orcas'\n",
    "\n",
    "# plot cleaned aol data or not\n",
    "cleaned_aol = False\n",
    "# cleaned_aol = True\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('queries')\n",
    "struc_level.append('named-entities')\n",
    "struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "base_analyses.append('character-count-frequencies') #-{struc}\n",
    "base_analyses.append('word-count-frequencies')\n",
    "base_analyses.append('entity-count-frequencies')\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        if item == 'word-count-frequencies' and struc != 'words':\n",
    "            analyses.append(f'{item}-{struc}')\n",
    "        elif item == 'entity-count-frequencies' and struc != 'named-entities' and struc != 'words':\n",
    "            analyses.append(f'{item}-{struc}')\n",
    "        elif item == 'character-count-frequencies':\n",
    "            analyses.append(f'{item}-{struc}')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-queries-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.23538933595021566 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.37209360202153524 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.17004571755727133 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.17229437430699665 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-named-entities-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.10616446733474731 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.2245997707049052 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.11634835799535116 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.09977917273839315 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-words-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.08730146090189615 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.14036491711934407 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.06896076997121176 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.056035550435384114 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-word-count-frequencies-queries-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.03248995542526245 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.10592023134231568 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.039378809928894046 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.02785459756851196 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-word-count-frequencies-named-entities-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.020679255326588947 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.04072564442952474 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.01649611790974935 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.015511810779571533 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-entity-count-frequencies-queries-special\n",
      "Loading data from aol dataset\n",
      "aol loaded in 0.006039114793141683 min\n",
      "Loading data from aql dataset\n",
      "aql loaded in 0.02189900477727254 min\n",
      "Loading data from ms-marco dataset\n",
      "ms-marco loaded in 0.0064715345700581866 min\n",
      "Loading data from orcas dataset\n",
      "orcas loaded in 0.01160959800084432 min\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pathlib import Path\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "    if dataset is None:\n",
    "        result_data = {}\n",
    "        # crawl files from all datasets and load into dictionary\n",
    "        paths = {f\"{name}\": _get_results_paths(name, analysis_name, cleaned_aql=True) for name in [\n",
    "            \"aol\", \"aql\", \"ms-marco\", \"orcas\"]}\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            paths[\"aol\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        if analysis_name == 'character-count-frequencies-queries':\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-char-count-special\")\n",
    "            paths[\"aql\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "            \n",
    "        for name, result_paths in paths.items():\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading data from {name} dataset\")\n",
    "            vis_data = load_results(result_paths, test_data=test_data)\n",
    "            result_data.update({name: vis_data})\n",
    "            end_time = time.time()  \n",
    "            print(f\"{name} loaded in {(end_time - start_time)/60} min\")\n",
    "\n",
    "        analysis_data.append(result_data)\n",
    "    else:\n",
    "        # load data from single dataset\n",
    "        result_paths = _get_results_paths(dataset, analysis_name, cleaned_aql=True)\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            result_paths = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from {dataset} dataset\")\n",
    "        result_data = {dataset: load_results(result_paths)}\n",
    "        end_time = time.time()  \n",
    "        print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "        analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character-count-frequencies-queries\n",
      "Test AQL vs AOL: ks-test-statistic: 0.11173531961695468 threshold: 0.00024166922498427152 common distribution: False\n",
      "Test AQL vs AOL: chi2-test-statistic: 65750470.63159055 threshold: 65.17076890356984 common distribution: False\n",
      "Test AQL vs MS-MARCO: ks-test-statistic: 0.12681904097342817 threshold: 0.0004602167429355898 common distribution: False\n",
      "Test AQL vs MS-MARCO: chi2-test-statistic: 308275880.3753942 threshold: 65.17076890356984 common distribution: False\n",
      "Test AQL vs ORCAS: ks-test-statistic: 0.2631041848885969 threshold: 0.000430322838737808 common distribution: False\n",
      "Test AQL vs ORCAS: chi2-test-statistic: 4828928069.247391 threshold: 65.17076890356984 common distribution: False\n",
      "character-count-frequencies-named-entities\n",
      "Test AQL vs AOL: ks-test-statistic: 0.08229342045206064 threshold: 0.0012005150337576981 common distribution: False\n",
      "Test AQL vs AOL: chi2-test-statistic: 2019180.2128319456 threshold: 65.17076890356984 common distribution: False\n",
      "Test AQL vs MS-MARCO: ks-test-statistic: 0.056895610807851216 threshold: 0.0010096337302523697 common distribution: False\n",
      "Test AQL vs MS-MARCO: chi2-test-statistic: 2373115.4987607715 threshold: 65.17076890356984 common distribution: False\n",
      "Test AQL vs ORCAS: ks-test-statistic: 0.032782918382632364 threshold: 0.0014805832635053453 common distribution: False\n",
      "Test AQL vs ORCAS: chi2-test-statistic: 2461300.0558739384 threshold: 65.17076890356984 common distribution: False\n",
      "character-count-frequencies-words\n",
      "Test AQL vs AOL: ks-test-statistic: 0.16309236611214406 threshold: 0.0007565753171732929 common distribution: False\n",
      "Test AQL vs AOL: chi2-test-statistic: 14914264.597192114 threshold: 65.17076890356984 common distribution: False\n",
      "Test AQL vs MS-MARCO: ks-test-statistic: 0.28424796781921646 threshold: 0.0007181740018955415 common distribution: False\n",
      "Test AQL vs MS-MARCO: chi2-test-statistic: 30480205.730806015 threshold: 65.17076890356984 common distribution: False\n",
      "Test AQL vs ORCAS: ks-test-statistic: 0.1663756524765042 threshold: 0.001235610638976858 common distribution: False\n",
      "Test AQL vs ORCAS: chi2-test-statistic: 6680797.786871629 threshold: 65.17076890356984 common distribution: False\n",
      "word-count-frequencies-queries\n",
      "Test AQL vs AOL: ks-test-statistic: 0.08449618850673357 threshold: 0.0002456506661322822 common distribution: False\n",
      "Test AQL vs AOL: chi2-test-statistic: 8984332.747542784 threshold: 12.59158724374398 common distribution: False\n",
      "Test AQL vs MS-MARCO: ks-test-statistic: 0.21408525810722387 threshold: 0.0004624826679478749 common distribution: False\n",
      "Test AQL vs MS-MARCO: chi2-test-statistic: 73334526.34194434 threshold: 12.59158724374398 common distribution: False\n",
      "Test AQL vs ORCAS: ks-test-statistic: 0.3292480267666995 threshold: 0.00043291572431038036 common distribution: False\n",
      "Test AQL vs ORCAS: chi2-test-statistic: 300539775.0489773 threshold: 12.59158724374398 common distribution: False\n",
      "word-count-frequencies-named-entities\n",
      "Test AQL vs AOL: ks-test-statistic: 0.06563109816072404 threshold: 0.001198825995718674 common distribution: False\n",
      "Test AQL vs AOL: chi2-test-statistic: 1694123.0208057822 threshold: 12.59158724374398 common distribution: False\n",
      "Test AQL vs MS-MARCO: ks-test-statistic: 0.042512803169886315 threshold: 0.0010084942783068845 common distribution: False\n",
      "Test AQL vs MS-MARCO: chi2-test-statistic: 982515.7676259786 threshold: 12.59158724374398 common distribution: False\n",
      "Test AQL vs ORCAS: ks-test-statistic: 0.13135691506522246 threshold: 0.0014794737007324534 common distribution: False\n",
      "Test AQL vs ORCAS: chi2-test-statistic: 5219850.508322846 threshold: 12.59158724374398 common distribution: False\n",
      "entity-count-frequencies-queries\n",
      "Test AQL vs AOL: ks-test-statistic: 0.08518556444657943 threshold: 0.00024051441248663577 common distribution: False\n",
      "Test AQL vs AOL: chi2-test-statistic: 10204019.001227675 threshold: 5.991464547107983 common distribution: False\n",
      "Test AQL vs MS-MARCO: ks-test-statistic: 0.0064311022974974374 threshold: 0.00045674815388229003 common distribution: False\n",
      "Test AQL vs MS-MARCO: chi2-test-statistic: 52693.8302029842 threshold: 5.991464547107983 common distribution: False\n",
      "Test AQL vs ORCAS: ks-test-statistic: 0.05619378739349068 threshold: 0.00042995196384475724 common distribution: False\n",
      "Test AQL vs ORCAS: chi2-test-statistic: 4130996.4079161407 threshold: 5.991464547107983 common distribution: False\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import ks_test, chi2_fit\n",
    "\n",
    "cnt = 0\n",
    "for analysis in analysis_data:\n",
    "    print(analyses[cnt])\n",
    "    test_data = analysis['aql']\n",
    "    test_counts = []\n",
    "    if 'character-count' in test_data.columns:\n",
    "        # print(\"character-count\")\n",
    "        test_data = test_data.query('`character-count` > 0')\n",
    "        test_data = test_data.query('`character-count` < 50').sort_values('character-count', ascending=True)\n",
    "        test_data = test_data['count()'].to_numpy()\n",
    "        col = 'character-count'\n",
    "        max = 50\n",
    "    elif 'entity-count' in test_data.columns:\n",
    "        # print(\"entity-count\")\n",
    "        test_data = test_data.query('`entity-count` >= 0')\n",
    "        test_data = test_data.query('`entity-count` < 3').sort_values('entity-count', ascending=True)\n",
    "        test_data = test_data['count()'].to_numpy()\n",
    "        col = 'entity-count'\n",
    "        max = 3\n",
    "    elif 'word-count' in test_data.columns:\n",
    "        # print(\"word-count\")\n",
    "        test_data = test_data.query('`word-count` > 0')\n",
    "        test_data = test_data.query('`word-count` < 8').sort_values('word-count', ascending=True)\n",
    "        test_data = test_data['count()'].to_numpy()  \n",
    "        col = 'word-count'\n",
    "        max = 8\n",
    "    # print(f\"max: {max}\")\n",
    "    for dataset, data in analysis.items():\n",
    "        if dataset != 'aql':\n",
    "            exp_counts = []\n",
    "            if col == 'entity-count':\n",
    "                exp_data = data.query(f\"`{col}` >= 0\")\n",
    "            else:\n",
    "                exp_data = data.query(f\"`{col}` > 0\")\n",
    "            exp_data = exp_data.query(f'`{col}` < {max}').sort_values(col, ascending=True)\n",
    "            exp_data = exp_data['count()'].to_numpy()\n",
    "            \n",
    "            test_statistic, threshold, test_res = ks_test(test_data, exp_data, significance_lvl=0.05)\n",
    "            print(f\"Test AQL vs {dataset.upper()}: ks-test-statistic: {test_statistic} threshold: {threshold} common distribution: {test_res}\")\n",
    "            test_statistic, threshold, test_res = chi2_fit(test_data, exp_data, significance_lvl=0.05)\n",
    "            print(f\"Test AQL vs {dataset.upper()}: chi2-test-statistic: {test_statistic} threshold: {threshold} common distribution: {test_res}\")\n",
    "\n",
    "    \n",
    "\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wasserstein Metric\n",
    "As we can see, the test clearly rejects the distributions being from the same underlying distribution. However, visualizations of the distributions showed that there are similarities. To assess, if the AQL's queries suit to the other realistic query logs, we measure distances between the distributions by computing the Wasserstein Metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pyarrow import compute as pc\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('queries')\n",
    "struc_level.append('named-Entities')\n",
    "struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "base_analyses.append('character-count') #-{struc}\n",
    "base_analyses.append('word-count')\n",
    "base_analyses.append('entity-count')\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        analyses.append(f'{item.lower()}-frequencies-{struc.lower()}')\n",
    "    del struc_level[-1]\n",
    "\n",
    "cleaned_aql = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading \"character-count-frequencies-queries\"\n",
      "AOL loaded in 0.35663317839304604 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-queries-special\n",
      "AQL loaded in 0.9555813868840536 min\n",
      "MS-MARCO loaded in 0.2065529505411784 min\n",
      "ORCAS loaded in 0.14613961776097614 min\n",
      "Start loading \"character-count-frequencies-named-entities\"\n",
      "AOL loaded in 0.1673794428507487 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-named-entities-special\n",
      "AQL loaded in 0.2386418581008911 min\n",
      "MS-MARCO loaded in 0.11655310392379761 min\n",
      "ORCAS loaded in 0.14881667693456013 min\n",
      "Start loading \"character-count-frequencies-words\"\n",
      "AOL loaded in 0.09455159902572632 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-character-count-frequencies-words-special\n",
      "AQL loaded in 0.15782722234725952 min\n",
      "MS-MARCO loaded in 0.052733282248179116 min\n",
      "ORCAS loaded in 0.062456834316253665 min\n",
      "Start loading \"word-count-frequencies-queries\"\n",
      "AOL loaded in 0.03332105080286662 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-word-count-frequencies-queries-special\n",
      "AQL loaded in 0.10317930777867636 min\n",
      "MS-MARCO loaded in 0.08918993473052979 min\n",
      "ORCAS loaded in 0.02804205020268758 min\n",
      "Start loading \"word-count-frequencies-named-entities\"\n",
      "AOL loaded in 0.018865342934926352 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-word-count-frequencies-named-entities-special\n",
      "AQL loaded in 0.05913161039352417 min\n",
      "MS-MARCO loaded in 0.03522869348526001 min\n",
      "ORCAS loaded in 0.016342671712239583 min\n",
      "Start loading \"entity-count-frequencies-queries\"\n",
      "AOL loaded in 0.008513017495473226 min\n",
      "/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-entity-count-frequencies-queries-special\n",
      "AQL loaded in 0.028095078468322755 min\n",
      "MS-MARCO loaded in 0.009815410772959391 min\n",
      "ORCAS loaded in 0.013674139976501465 min\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# load data\n",
    "analysis_data = {}\n",
    "for analysis in analyses:\n",
    "    print(f\"Start loading \\\"{analysis}\\\"\")\n",
    "    datasets = {}\n",
    "    for dataset in [\"aol\",\"aql\",\"ms-marco\",\"orcas\"]:\n",
    "        start_time = time()\n",
    "        paths = _get_results_paths(dataset, analysis, cleaned_aql)\n",
    "        result_data = load_results(paths)\n",
    "        datasets.update({dataset: result_data})\n",
    "        end_time = time()\n",
    "        print(f\"{dataset.upper()} loaded in {(end_time - start_time)/60} min\")\n",
    "    analysis_data.update({analysis: datasets})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Wasserstein Distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character-count-frequencies-queries\n",
      "           aol   aql  ms-marco  orcas\n",
      "aol       0.00  5.54      2.56   3.72\n",
      "aql       5.54  0.00      4.75   7.75\n",
      "ms-marco  2.56  4.75      0.00   3.00\n",
      "orcas     3.72  7.75      3.00   0.00\n",
      "character-count-frequencies-named-entities\n",
      "           aol   aql  ms-marco  orcas\n",
      "aol       0.00  2.12      1.79   0.80\n",
      "aql       2.12  0.00      2.86   1.98\n",
      "ms-marco  1.79  2.86      0.00   1.03\n",
      "orcas     0.80  1.98      1.03   0.00\n",
      "character-count-frequencies-words\n",
      "           aol   aql  ms-marco  orcas\n",
      "aol       0.00  3.69      6.31   4.20\n",
      "aql       3.69  0.00      6.29   4.17\n",
      "ms-marco  6.31  6.29      0.00   2.20\n",
      "orcas     4.20  4.17      2.20   0.00\n",
      "word-count-frequencies-queries\n",
      "           aol   aql  ms-marco  orcas\n",
      "aol       0.00  0.58      0.69   0.89\n",
      "aql       0.58  0.00      0.81   1.25\n",
      "ms-marco  0.69  0.81      0.00   0.44\n",
      "orcas     0.89  1.25      0.44   0.00\n",
      "word-count-frequencies-named-entities\n",
      "           aol   aql  ms-marco  orcas\n",
      "aol       0.00  0.19      0.06   0.11\n",
      "aql       0.19  0.00      0.18   0.30\n",
      "ms-marco  0.06  0.18      0.00   0.17\n",
      "orcas     0.11  0.30      0.17   0.00\n",
      "entity-count-frequencies-queries\n",
      "           aol   aql  ms-marco  orcas\n",
      "aol       0.00  0.18      0.12   0.03\n",
      "aql       0.18  0.00      0.06   0.15\n",
      "ms-marco  0.12  0.06      0.00   0.09\n",
      "orcas     0.03  0.15      0.09   0.00\n"
     ]
    }
   ],
   "source": [
    "from thesis_schneg.vis_modules import get_max_x\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "cnt=0\n",
    "distances_data = {}\n",
    "for analysis, datasets in analysis_data.items():\n",
    "    x_max = get_max_x(datasets, f\"{analysis.split('-')[0]}-count\")\n",
    "    distances = DataFrame(np.zeros((len(datasets), len(datasets))), index=datasets.keys(), columns=datasets.keys())\n",
    "    names = []\n",
    "    j=0\n",
    "    for dataset_name, data in datasets.items():\n",
    "        data1 = {dataset_name: data.sort_values(f\"{analysis.split('-')[0]}-count\", ascending=True)}\n",
    "        names.append(dataset_name)\n",
    "        x_vals = data1[dataset_name][f\"{analysis.split('-')[0]}-count\"][0:x_max]\n",
    "        y_vals = data1[dataset_name]['count()'][0:x_max]\n",
    "        i=0\n",
    "        for dataset_name, data in datasets.items():\n",
    "            if dataset_name in names:\n",
    "                dist = 0\n",
    "            else:\n",
    "                data2 = data.sort_values(f\"{analysis.split('-')[0]}-count\", ascending=True)\n",
    "                x_vals2 = data2[f\"{analysis.split('-')[0]}-count\"][0:x_max]\n",
    "                y_vals2 = data2['count()'][0:x_max]\n",
    "                dist = wasserstein_distance(x_vals, x_vals2, u_weights=y_vals, v_weights=y_vals2)\n",
    "            distances.iloc[i, j] = round(dist,2)\n",
    "            i+=1\n",
    "        j+=1\n",
    "    distances = distances + distances.T\n",
    "    distances_data.update({analysis: distances})\n",
    "    cnt+=1\n",
    "for key, value in distances_data.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "\n",
    "## get avarage wasserstein distances per query log\n",
    "# avg_distances = {}\n",
    "# for analysis, distances in distances_data.items():\n",
    "#     avg_distances.update({analysis: distances.mean()})\n",
    "    \n",
    "# for key, value in avg_distances.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Distances by applying [MDS-transformation](https://en.wikipedia.org/wiki/Multidimensional_scaling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character-count-frequencies-queries\n",
      "character-count-frequencies-named-entities\n",
      "character-count-frequencies-words\n",
      "word-count-frequencies-queries\n",
      "word-count-frequencies-named-entities\n",
      "entity-count-frequencies-queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74651/3814933885.py:61: UserWarning: FigureCanvasPgf is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from matplotlib import pyplot as plt, use\n",
    "from pathlib import Path    \n",
    "import scienceplots\n",
    "\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"grid\"])\n",
    "\n",
    "save_vis = True\n",
    "# save_vis = False\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "trans_dists = {}\n",
    "color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "for key, value in distances_data.items():\n",
    "    print(key)\n",
    "    trans_dists.update({key: mds.fit_transform(value)})\n",
    "\n",
    "num_plots = len(trans_dists)\n",
    "cols = 3\n",
    "rows = num_plots//cols\n",
    "if num_plots % cols != 0:\n",
    "    rows+=1\n",
    "\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio\n",
    "\n",
    "# fig,ax = plt.subplots(rows,cols, figsize=(4*cols,4*rows))\n",
    "fig,ax = plt.subplots(rows,cols, figsize=(width,height))\n",
    "\n",
    "# for key, value in trans_dists.items() and ax in ax.flatten():\n",
    "key = [key for key in trans_dists.keys()]\n",
    "# title_list = []\n",
    "# for i in key:\n",
    "#     title_strings = {}\n",
    "#     if key.split('-')[0] == 'character':\n",
    "#         title_strings.update({key.split('-')[0]: [key]})\n",
    "axes = ax.flatten()\n",
    "for i in range(len(trans_dists)):\n",
    "    ax = axes[i]\n",
    "    value = trans_dists[key[i]]\n",
    "    # plt.figure()\n",
    "    for j in range(len(value)):\n",
    "        ax.scatter(value[j][0], value[j][1], c=color[j], label=names[j].upper())\n",
    "    # plt.scatter(trans_dists[key][:,0],trans_dists[key][:,1], c=color, label=names)\n",
    "    # ax.title(\"MDS Plot of Wasserstein Distances\")\n",
    "    # plt.legend()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"{key[i].split('-')[-1].title()} in {key[i].split('-')[0].title()}s\")\n",
    "    # ax.yticks([])\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels,fancybox=True,framealpha=0.5).get_frame().set_linewidth(0.5)\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.01), fancybox=False, ncol=len(names),edgecolor=\"black\").get_frame().set_linewidth(0.5)\n",
    "\n",
    "# fig.suptitle(\"MDS Plot of Wasserstein Distances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "\n",
    "vis_dir = Path(f\"{base_path}Wasserstein-Distances-Lengths\")\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "if save_vis:\n",
    "    use(\"pgf\")\n",
    "    fig.savefig(vis_dir.joinpath(\"all.pgf\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of the resulting distributions\n",
    "In this notebook we want to compare the resulting distributions by applying various statistical methods to quantify differences and similarities.\n",
    "## Frequencies of Linguistic Elements\n",
    "To assess the similarity of the resulting distributions, we compute the [*Wasserstein-Metric*](https://en.wikipedia.org/wiki/Wasserstein_metric) of each pair of distributions.\n",
    "The Wasserstein-Metric is a distance function for probability distributions. It satisfies the criteria of actual mathematical metrics and thus enables comparisons between probability distributions. Assuming one-dimensional distributions, the Wasserstein Distance of two empirical samples $P$ and $Q$ with respective random variables $X_1,...,X_n$ and $Y_1,...,Y_n$ is given by:\n",
    "$$ W_{p}(P,Q)=\\left({\\frac {1}{n}}\\sum _{i=1}^{n}\\|X_{(i)}-Y_{(i)}\\|^{p}\\right)^{\\frac {1}{p}}$$ \n",
    "Let's compute the Wasserstein-Metric for the frequencies of Characters, Words, Named Entities and Queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "import numpy as np\n",
    "\n",
    "analyses = []\n",
    "# set analysis that should be analyzed\n",
    "# analyses.append('query-frequencies')\n",
    "analyses.append('extract-named-entities')\n",
    "analyses.append('extract-words')\n",
    "analyses.append('extract-chars')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "cleaned_aql = True\n",
    "\n",
    "col = ['count()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "# load data\n",
    "analysis_data = {}\n",
    "for analysis in analyses:\n",
    "    print(f\"Start loading \\\"{analysis}\\\"\")\n",
    "    datasets = {}\n",
    "    for dataset in [\"aol\",\"aql\",\"ms-marco\",\"orcas\"]:\n",
    "        start_time = time()\n",
    "        paths = _get_results_paths(dataset, analysis, cleaned_aql)\n",
    "        result_data = load_results(paths, test_data=test_data, cols=col)\n",
    "        datasets.update({dataset: result_data})\n",
    "        end_time = time()\n",
    "        print(f\"{dataset.upper()} loaded in {(end_time - start_time)/60} min\")\n",
    "    analysis_data.update({analysis: datasets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import get_max_x\n",
    "threshs = [0.84, 0.957, 0.999]\n",
    "threshs = [0.99, 0.99, 0.999]\n",
    "\n",
    "data = analysis_data[analyses[2]]['aql'] \n",
    "print(len(data))\n",
    "x_max = get_max_x(analysis_data[analyses[2]], \"count()\", rank_sized=True, threshold=0.999)\n",
    "print(x_max)\n",
    "\n",
    "for key,val in analysis_data[analyses[2]].items():\n",
    "    print(key)\n",
    "    # print(val)\n",
    "    print(len(val))\n",
    "\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Wasserstein-Distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import get_max_x\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import wasserstein_distance_nd\n",
    "\n",
    "# we choose thresholds so that the aqls elemntes are aligned with the highest number of elements present in the other datasets\n",
    "threshs = [0.84, 0.957, 0.999]\n",
    "# threshs = [0.99, 0.99, 0.999]\n",
    "\n",
    "cnt=0\n",
    "distances_data = {}\n",
    "for analysis, datasets in analysis_data.items():\n",
    "    print(analysis)\n",
    "    x_max = get_max_x(datasets, \"count()\", rank_sized=True, threshold=threshs[cnt])\n",
    "    print(x_max)\n",
    "    # distances = ndarray((len(datasets), len(datasets)))\n",
    "    distances = DataFrame(np.zeros((len(datasets), len(datasets))), index=datasets.keys(), columns=datasets.keys())\n",
    "    names = []\n",
    "    j=0\n",
    "    for dataset_name, data in datasets.items():\n",
    "        data1 = data.sort_values(\"count()\", ascending=False)/data['count()'].sum()\n",
    "        # print(data1[\"count()\"].iloc[0:3])\n",
    "        # print(data1[\"count()\"].iloc[-3:-1])\n",
    "\n",
    "        # data1 = data.sort_values(\"count()\", ascending=True)\n",
    "        names.append(dataset_name)\n",
    "        x_vals = np.arange(1, x_max+1)\n",
    "        y_vals = data1['count()'][0:x_max].to_numpy()\n",
    "        # \n",
    "        if len(y_vals) < x_max:\n",
    "            y_vals = np.append(y_vals, np.zeros(x_max-len(y_vals)))\n",
    "        i=0\n",
    "        for dataset_name, data in datasets.items():\n",
    "            if dataset_name in names:\n",
    "                dist = 0\n",
    "            else:\n",
    "                data2 = data.sort_values(\"count()\", ascending=False)/data['count()'].sum()\n",
    "                # data2 = data.sort_values(\"count()\", ascending=True)\n",
    "                x_vals2 = np.arange(1, x_max+1)\n",
    "                y_vals2 = data2['count()'][0:x_max].to_numpy()\n",
    "                if len(y_vals2) < x_max:\n",
    "                    y_vals2 = np.append(y_vals2, np.zeros(x_max-len(y_vals2)))\n",
    "                dist = wasserstein_distance_nd(x_vals, x_vals2, u_weights=y_vals, v_weights=y_vals2)\n",
    "            # distances[i][j] = dist\n",
    "            distances.iloc[i, j] = dist\n",
    "            i+=1\n",
    "        j+=1\n",
    "    distances = distances + distances.T\n",
    "    distances_data.update({analysis: distances})\n",
    "    cnt+=1\n",
    "for key, value in distances_data.items():\n",
    "    print(value)\n",
    "\n",
    "## get avarage wasserstein distances per query log\n",
    "avg_distances = {}\n",
    "for analysis, distances in distances_data.items():\n",
    "    avg_distances.update({analysis: distances.mean().mean()})\n",
    "    \n",
    "for key, value in avg_distances.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average distance within AOL, MS-MARCO, ORCAS,\n",
    "# compute average distance between AQL and the other datasets\n",
    "# compute difference between the average distance of AQL and the average distance of the other datasets\n",
    "# compute std of the distances within comparison group\n",
    "# express difference in std\n",
    "def get_ws_stats(distancedata):\n",
    "    ws_stats = {}\n",
    "    for analysis, distances in distancedata.items():\n",
    "        # get distances of AOL, MS MARCO, ORCAS\n",
    "        aol = distances.loc['aol'].to_numpy()\n",
    "        # print(aol)\n",
    "        ms_marco = distances.loc['ms-marco'].to_numpy()\n",
    "        # get distances of AQL\n",
    "        aql = distances.loc['aql'].to_numpy()\n",
    "        # print(aol,aql,ms_marco,orcas,sep='\\n')\n",
    "        aql_dist = aql.sum()/3\n",
    "        notaql_dist = (aol[2]+aol[3]+ms_marco[3])/3\n",
    "        std = np.std([aol[2], aol[3], ms_marco[3]])\n",
    "        aql_dist_deviation = notaql_dist - aql_dist\n",
    "        aql_distinSTD = aql_dist_deviation/std\n",
    "        ws_stats.update({analysis: [notaql_dist, aql_dist, aql_dist_deviation, std, aql_distinSTD]})\n",
    "    return ws_stats\n",
    "        \n",
    "for key, value in get_ws_stats(distances_data).items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results in Wasserstein Space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "trans_dists = {}\n",
    "for key, value in distances_data.items():\n",
    "    print(key)\n",
    "    trans_dists.update({key: mds.fit_transform(value)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path    \n",
    "import scienceplots\n",
    "\n",
    "def avg_dist_notAQL(x,y, point_matrtix):\n",
    "    dist = 0\n",
    "    for i in range(len(point_matrtix)):\n",
    "        if i != 1:\n",
    "            dist += np.sqrt((x - point_matrtix[i][0])**2 + (y - point_matrtix[i][1])**2)\n",
    "    dist = dist/3\n",
    "    # return dist\n",
    "    # # get the average distance of the points within AOL, ORCAS, MS-MARCO Web Search    \n",
    "    avg_dist_within = 0\n",
    "    for i in [(0,2), (0,3), (2,3)]:\n",
    "        avg_dist_within += np.sqrt((point_matrtix[i[0]][0] - point_matrtix[i[1]][0])**2 + (point_matrtix[i[0]][1] - point_matrtix[i[1]][1])**2)\n",
    "    avg_dist_within = avg_dist_within/3\n",
    "\n",
    "    # get the standard deviation of distances within AOL, ORCAS, MS-MARCO Web Search\n",
    "    std_dist_within = 0\n",
    "    for i in [(0,2), (0,3), (2,3)]:\n",
    "        std_dist_within += (np.sqrt((point_matrtix[i[0]][0] - point_matrtix[i[1]][0])**2 + (point_matrtix[i[0]][1] - point_matrtix[i[1]][1])**2) - avg_dist_within)**2\n",
    "    std_dist_within = np.sqrt(std_dist_within/3) \n",
    "\n",
    "    if dist <= avg_dist_within:\n",
    "        return avg_dist_within\n",
    "    elif dist > avg_dist_within and dist <= avg_dist_within + std_dist_within:\n",
    "        return avg_dist_within + std_dist_within\n",
    "    elif dist > avg_dist_within + std_dist_within and dist <= avg_dist_within + 2 * std_dist_within:\n",
    "        return avg_dist_within + 2 * std_dist_within\n",
    "    elif dist > avg_dist_within + 2 * std_dist_within and dist <= avg_dist_within + 3 * std_dist_within:\n",
    "        return avg_dist_within + 3 * std_dist_within\n",
    "    elif dist > avg_dist_within + 3 * std_dist_within and dist <= avg_dist_within + 4 * std_dist_within:\n",
    "        return avg_dist_within + 4 * std_dist_within\n",
    "    elif dist > avg_dist_within + 4 * std_dist_within and dist <= avg_dist_within + 5 * std_dist_within:\n",
    "        return avg_dist_within + 5 * std_dist_within\n",
    "    else: \n",
    "        return avg_dist_within + 6 * std_dist_within\n",
    "        \n",
    "def avg_dist_within(points):\n",
    "    dist = 0\n",
    "    pairs = [(0, 2), (0, 3), (2, 3)]  # Indizes der Punkte AOL, MS-MARCO, ORCAS\n",
    "    for i, j in pairs:\n",
    "        dist += np.sqrt((points[i][0] - points[j][0])**2 + (points[i][1] - points[j][1])**2)\n",
    "    return dist / len(pairs)\n",
    "def std_within(points):\n",
    "    dist = 0\n",
    "    pairs = [(0, 2), (0, 3), (2, 3)]  # Indizes der Punkte AOL, MS-MARCO, ORCAS\n",
    "    for i, j in pairs:\n",
    "        dist += (np.sqrt((points[i][0] - points[j][0])**2 + (points[i][1] - points[j][1])**2) - avg_dist_within(points))**2\n",
    "    return np.sqrt(dist / len(pairs))\n",
    "\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"ieee\"])\n",
    "color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "\n",
    "save_vis = True\n",
    "# save_vis = False\n",
    "plot_legend = True\n",
    "# plot_legend = False\n",
    "\n",
    "# get meshsize\n",
    "# mesh_size = 70\n",
    "mesh_size = 500\n",
    "\n",
    "num_plots = len(trans_dists)\n",
    "cols = 3\n",
    "rows = num_plots//cols\n",
    "if num_plots % cols != 0:\n",
    "    rows+=1\n",
    "\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale *1.1\n",
    "height = width * aspect_ratio * 0.40\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(rows,cols, figsize=(width,height))\n",
    "\n",
    "\n",
    "# for making the titles\n",
    "keys = [key for key in trans_dists.keys()]\n",
    "# Dynamically generate titles for each plot\n",
    "titles = [f\"{key.split('-')[-1].title()} in {key.split('-')[0].title()}s\" for key in keys]\n",
    "# Adjust titles for specific cases (e.g., \"Entities\" instead of \"Entitys\")\n",
    "titles = [\"Entities ranked\", \"Words ranked\", \"Chars ranked\"]\n",
    "print(titles)\n",
    "\n",
    "positions = [[0.0,0.0], [1.0,0.0], [0.0,0.0]]\n",
    "\n",
    "\n",
    "axes = ax.flatten()\n",
    "for i in range(len(trans_dists)):\n",
    "    ax = axes[i]\n",
    "    ax.set_title(titles[i], fontsize=9)\n",
    "\n",
    "    points = trans_dists[keys[i]]\n",
    "\n",
    "    # get the highes and lowest x and y values of the points\n",
    "    x_min = np.min(points[:,0])\n",
    "    x_max = np.max(points[:,0])\n",
    "    y_min = np.min(points[:,1])\n",
    "    y_max = np.max(points[:,1])\n",
    "\n",
    "    # scale the extreme values of x and y to get a good boundary for the plot\n",
    "    # we take the scale of the min range\n",
    "    range_scale = 0.4\n",
    "    x_min = x_min - range_scale * (x_max - x_min)\n",
    "    x_max = x_max + range_scale * (x_max - x_min)\n",
    "    y_min = y_min - range_scale * (y_max - y_min)\n",
    "    y_max = y_max + range_scale * (y_max - y_min)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, mesh_size)\n",
    "    y = np.linspace(y_min, y_max, mesh_size)\n",
    "\n",
    "    z = np.array([avg_dist_notAQL(i,j,points) for j in y for i in x])\n",
    "\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = z.reshape(mesh_size, mesh_size)\n",
    "\n",
    "    avg_dist = avg_dist_within(points)\n",
    "    std_dist = std_within(points)\n",
    "\n",
    "    ax.contourf(X.astype(float), Y.astype(float), Z.astype(float), 10, alpha=0.7, cmap = 'Blues', linewidths=0.5, linestyles='solid') #, cmap='Blues' 'OrRd'\n",
    "    # Add contour lines for the average distance within AOL, MS-MARCO, and ORCAS and the standard deviation \n",
    "    contour = ax.contour(X.astype(float), Y.astype(float), Z.astype(float), levels=[avg_dist, avg_dist+std_dist, avg_dist+2*std_dist, avg_dist+3*std_dist, avg_dist+4*std_dist, avg_dist+5*std_dist], colors='black', linewidths=0.5, linestyles='dashed')\n",
    "\n",
    "    # Label the contour lines with descriptive text\n",
    "    text = [\"$d_{{\\mu}}$\", \"$d_{{\\mu}} + \\sigma$\", \"$d_{{\\mu}} + 2 \\sigma$\"]\n",
    "    for j, level in enumerate(contour.levels):\n",
    "        # Find the position for the label (e.g., the midpoint of the contour line)\n",
    "        idx = np.where(Z == level)\n",
    "        if len(idx[0]) > 0:\n",
    "            if j == 0:\n",
    "                x_pos = positions[i][0] \n",
    "                y_pos = positions[i][1]\n",
    "                ax.text(x_pos, y_pos, text[j] , fontsize=8, color='black')\n",
    "\n",
    "    labels = [name.upper() for name in names]\n",
    "    labels[2] = \"MS-MARCO WS\"\n",
    "    # plot 3d scatter plot\n",
    "    for j in range(len(points)):\n",
    "        ax.scatter(points[j][0], points[j][1], c=color[j], label=labels[j], s=15, edgecolor='black', linewidth=0.3, alpha=0.7)\n",
    "\n",
    "    ax.grid(True)\n",
    "    ax.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\n",
    "    ax.xaxis.offsetText.set_x(1.1)  # Verschiebt den Exponenten horizontal\n",
    "    ax.xaxis.offsetText.set_y(-0.9)  # Verschiebt den Exponenten vertikal\n",
    "    \n",
    "if plot_legend:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.04), fancybox=False, ncol=len(names),edgecolor=\"black\", frameon=True).get_frame().set_linewidth(0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.9, hspace=0.4, wspace=0.3)\n",
    "# plt.show()\n",
    "\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "if plot_legend:\n",
    "    vis_dir = Path(f\"{base_path}Wasserstein-Distances-elements-2d-contour\")\n",
    "else:\n",
    "    vis_dir = Path(f\"{base_path}Wasserstein-Distances-elements-2d-contour-no-legend\")\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "if save_vis:\n",
    "    fig.savefig(vis_dir.joinpath(\"all.pdf\"), format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length-related Frequencies\n",
    "### 1. Kolmogerov-Smirnov-Test\n",
    "First of all, we carry out a statistical test to check if the different distributions result from a common underlying distribution. We select the [*Kolmogerov-Smirnov-Test*](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test) to verify this hypothesis.  \n",
    "\n",
    "The test computes a test statistic $D$ that is compared to a threshold $D_{\\alpha}$ with a significance level $\\alpha$. If\n",
    "$$D < D_{\\alpha}$$ \n",
    "then the two distributions are likely to be similar. If, in contrast, \n",
    "$$D \\ge D_{\\alpha}$$ \n",
    "then we can conclude the samples are not from a common distribution with a significance of $1-\\alpha$. The test measures the maximum distance of the cumulative distribution functions $F$ of the involved distributions. Accordingly, $D$ is computed by\n",
    "$$D = \\mathrm{max} |( F_{1} - F_{2} )| $$\n",
    "The threshold can be determined by \n",
    "$$D_{\\alpha} = K_{\\alpha}\\sqrt{(n_1+n_2)/(n_1 \\cdot n_2)} $$\n",
    "$K_{\\alpha}$ is a constant that is dependent on the significance level $\\alpha$ and can be obtained by this table:\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "\n",
    "##### set visualization configuration #####\n",
    "\n",
    "\n",
    "\n",
    "# select dataset (if none is selected all datasets are visualized in a joint plot) \n",
    "dataset: DatasetName = None \n",
    "# dataset: DatasetName = 'aol'\n",
    "# dataset: DatasetName = 'aql'\n",
    "# dataset: DatasetName = 'ms-marco'\n",
    "# dataset: DatasetName = 'orcas'\n",
    "\n",
    "# plot cleaned aol data or not\n",
    "cleaned_aol = False\n",
    "# cleaned_aol = True\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('queries')\n",
    "struc_level.append('named-entities')\n",
    "struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "base_analyses.append('character-count-frequencies') #-{struc}\n",
    "base_analyses.append('word-count-frequencies')\n",
    "base_analyses.append('entity-count-frequencies')\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        if item == 'word-count-frequencies' and struc != 'words':\n",
    "            analyses.append(f'{item}-{struc}')\n",
    "        elif item == 'entity-count-frequencies' and struc != 'named-entities' and struc != 'words':\n",
    "            analyses.append(f'{item}-{struc}')\n",
    "        elif item == 'character-count-frequencies':\n",
    "            analyses.append(f'{item}-{struc}')\n",
    "\n",
    "# test_data = True\n",
    "test_data = False\n",
    "\n",
    "normalize_data = True\n",
    "# normalize_data = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pathlib import Path\n",
    "import time\n",
    "color = None\n",
    "label = None\n",
    "# load data\n",
    "analysis_data = []\n",
    "for analysis_name in analyses:\n",
    "    if dataset is None:\n",
    "        result_data = {}\n",
    "        # crawl files from all datasets and load into dictionary\n",
    "        paths = {f\"{name}\": _get_results_paths(name, analysis_name, cleaned_aql=True) for name in [\n",
    "            \"aol\", \"aql\", \"ms-marco\", \"orcas\"]}\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            paths[\"aol\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        if analysis_name == 'character-count-frequencies-queries':\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aql-get-char-count-special\")\n",
    "            paths[\"aql\"] = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "            \n",
    "        for name, result_paths in paths.items():\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading data from {name} dataset\")\n",
    "            vis_data = load_results(result_paths, test_data=test_data)\n",
    "            result_data.update({name: vis_data})\n",
    "            end_time = time.time()  \n",
    "            print(f\"{name} loaded in {(end_time - start_time)/60} min\")\n",
    "\n",
    "        analysis_data.append(result_data)\n",
    "    else:\n",
    "        # load data from single dataset\n",
    "        result_paths = _get_results_paths(dataset, analysis_name, cleaned_aql=True)\n",
    "        if analysis_name == 'character-count-frequencies-words' and cleaned_aol:\n",
    "            base_path = Path(\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/aol-words-character-count-frequencies-special\")\n",
    "            result_paths = [path for path in base_path.iterdir(\n",
    "        )]\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from {dataset} dataset\")\n",
    "        result_data = {dataset: load_results(result_paths)}\n",
    "        end_time = time.time()  \n",
    "        print(f\"{dataset} loaded in {(end_time - start_time)/60} min\")\n",
    "        analysis_data.append(result_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import ks_test, chi2_fit\n",
    "\n",
    "cnt = 0\n",
    "for analysis in analysis_data:\n",
    "    print(analyses[cnt])\n",
    "    test_data = analysis['aql']\n",
    "    test_counts = []\n",
    "    if 'character-count' in test_data.columns:\n",
    "        # print(\"character-count\")\n",
    "        test_data = test_data.query('`character-count` > 0')\n",
    "        test_data = test_data.query('`character-count` < 50').sort_values('character-count', ascending=True)\n",
    "        test_data = test_data['count()'].to_numpy()\n",
    "        col = 'character-count'\n",
    "        max = 50\n",
    "    elif 'entity-count' in test_data.columns: \n",
    "        # print(\"entity-count\")\n",
    "        test_data = test_data.query('`entity-count` >= 0')\n",
    "        test_data = test_data.query('`entity-count` < 3').sort_values('entity-count', ascending=True)\n",
    "        test_data = test_data['count()'].to_numpy()\n",
    "        col = 'entity-count'\n",
    "        max = 3\n",
    "    elif 'word-count' in test_data.columns:\n",
    "        # print(\"word-count\")\n",
    "        test_data = test_data.query('`word-count` > 0')\n",
    "        test_data = test_data.query('`word-count` < 8').sort_values('word-count', ascending=True)\n",
    "        test_data = test_data['count()'].to_numpy()  \n",
    "        col = 'word-count'\n",
    "        max = 8\n",
    "    # print(f\"max: {max}\")\n",
    "    for dataset, data in analysis.items():\n",
    "        if dataset != 'aql':\n",
    "            exp_counts = []\n",
    "            if col == 'entity-count':\n",
    "                exp_data = data.query(f\"`{col}` >= 0\")\n",
    "            else:\n",
    "                exp_data = data.query(f\"`{col}` > 0\")\n",
    "            exp_data = exp_data.query(f'`{col}` < {max}').sort_values(col, ascending=True)\n",
    "            exp_data = exp_data['count()'].to_numpy()\n",
    "            \n",
    "            test_statistic, threshold, test_res = ks_test(test_data, exp_data, significance_lvl=0.05)\n",
    "            print(f\"Test AQL vs {dataset.upper()}: ks-test-statistic: {test_statistic} threshold: {threshold} common distribution: {test_res}\")\n",
    "            test_statistic, threshold, test_res = chi2_fit(test_data, exp_data, significance_lvl=0.05)\n",
    "            print(f\"Test AQL vs {dataset.upper()}: chi2-test-statistic: {test_statistic} threshold: {threshold} common distribution: {test_res}\")\n",
    "\n",
    "    \n",
    "\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wasserstein Metric\n",
    "As we can see, the test clearly rejects the distributions being from the same underlying distribution. However, visualizations of the distributions showed that there are similarities. To assess, if the AQL's queries suit to the other realistic query logs, we measure distances between the distributions by computing the Wasserstein Metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.model import DatasetName\n",
    "from thesis_schneg.vis_modules import _get_results_paths, load_results\n",
    "from pyarrow import compute as pc\n",
    "import numpy as np\n",
    "\n",
    "struc_level = []\n",
    "struc_level.append('queries')\n",
    "struc_level.append('named-Entities')\n",
    "struc_level.append('words')\n",
    "\n",
    "base_analyses = []\n",
    "base_analyses.append('character-count') #-{struc}\n",
    "base_analyses.append('word-count')\n",
    "base_analyses.append('entity-count')\n",
    "\n",
    "analyses = []\n",
    "for item in base_analyses:\n",
    "    for struc in struc_level:\n",
    "        analyses.append(f'{item.lower()}-frequencies-{struc.lower()}')\n",
    "    del struc_level[-1]\n",
    "\n",
    "cleaned_aql = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "# load data\n",
    "analysis_data = {}\n",
    "for analysis in analyses:\n",
    "    print(f\"Start loading \\\"{analysis}\\\"\")\n",
    "    datasets = {}\n",
    "    for dataset in [\"aol\",\"aql\",\"ms-marco\",\"orcas\"]:\n",
    "        start_time = time()\n",
    "        paths = _get_results_paths(dataset, analysis, cleaned_aql)\n",
    "        result_data = load_results(paths)\n",
    "        datasets.update({dataset: result_data})\n",
    "        end_time = time()\n",
    "        print(f\"{dataset.upper()} loaded in {(end_time - start_time)/60} min\")\n",
    "    analysis_data.update({analysis: datasets})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Wasserstein Distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_schneg.vis_modules import get_max_x\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import wasserstein_distance_nd\n",
    "\n",
    "cnt=0\n",
    "distances_data = {}\n",
    "\n",
    "\n",
    "for analysis, datasets in analysis_data.items():\n",
    "    x_max = get_max_x(datasets, f\"{analysis.split('-')[0]}-count\")\n",
    "    distances = DataFrame(np.zeros((len(datasets), len(datasets))), index=datasets.keys(), columns=datasets.keys())\n",
    "    \n",
    "    names = []\n",
    "    j=0\n",
    "    for dataset_name, data in datasets.items():\n",
    "        data1 = {dataset_name: data.sort_values(f\"{analysis.split('-')[0]}-count\", ascending=True)}\n",
    "        names.append(dataset_name)\n",
    "        x_vals = data1[dataset_name][f\"{analysis.split('-')[0]}-count\"][0:x_max]\n",
    "        y_vals = data1[dataset_name]['count()'][0:x_max]\n",
    "        i=0\n",
    "        for dataset_name, data in datasets.items():\n",
    "            if dataset_name in names:\n",
    "                dist = 0\n",
    "            else:\n",
    "                data2 = data.sort_values(f\"{analysis.split('-')[0]}-count\", ascending=True)\n",
    "                x_vals2 = data2[f\"{analysis.split('-')[0]}-count\"][0:x_max]\n",
    "                y_vals2 = data2['count()'][0:x_max]\n",
    "                dist = wasserstein_distance_nd(x_vals, x_vals2, u_weights=y_vals, v_weights=y_vals2)\n",
    "\n",
    "            distances.iloc[i, j] = round(dist,2)\n",
    "\n",
    "            i+=1\n",
    "        j+=1\n",
    "    distances = distances + distances.T\n",
    "    distances_data.update({analysis: distances})\n",
    "    cnt+=1\n",
    "# for key, value in distances_data.items():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "## get avarage wasserstein distances in comparison group and of aql\n",
    "# avg_distances = {}\n",
    "for analysis, distances in distances_data.items():\n",
    "    aql_avg = distances['aql'].sum()/3\n",
    "    print(f\"AQLavg = {aql_avg}\")\n",
    "    notaql_avg = (distances['aol'][2:3].sum() + distances['ms-marco'][3].sum())/3\n",
    "    print(f\"notAQLavg = {notaql_avg}\")\n",
    "\n",
    "    \n",
    "    # get average wasserstein distances per query log\n",
    "    avg_distances = {}\n",
    "    avg_distances.update({analysis: distances.mean()})\n",
    "    \n",
    "for key, value in avg_distances.items():\n",
    "    print(key, value)\n",
    "\n",
    "# get standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average distance within AOL, MS-MARCO, ORCAS,\n",
    "# compute average distance between AQL and the other datasets\n",
    "# compute difference between the average distance of AQL and the average distance of the other datasets\n",
    "# compute std of the distances within comparison group\n",
    "# express difference in std\n",
    "def get_ws_stats(distancedata):\n",
    "    ws_stats = {}\n",
    "    for analysis, distances in distancedata.items():\n",
    "        # get distances of AOL, MS MARCO, ORCAS\n",
    "        aol = distances.loc['aol'].to_numpy()\n",
    "        # print(aol)\n",
    "        ms_marco = distances.loc['ms-marco'].to_numpy()\n",
    "        # get distances of AQL\n",
    "        aql = distances.loc['aql'].to_numpy()\n",
    "        # print(aol,aql,ms_marco,orcas,sep='\\n')\n",
    "        aql_dist = aql.sum()/3\n",
    "        notaql_dist = (aol[2]+aol[3]+ms_marco[3])/3\n",
    "        std = np.std([aol[2], aol[3], ms_marco[3]])\n",
    "        aql_dist_deviation = notaql_dist - aql_dist\n",
    "        aql_distinSTD = aql_dist_deviation/std\n",
    "        ws_stats.update({analysis: [notaql_dist, aql_dist, aql_dist_deviation, std, aql_distinSTD]})\n",
    "    return ws_stats\n",
    "        \n",
    "for key, value in get_ws_stats(distances_data).items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 4 distances for each characteristic, we can display a configuration of points in the three-dimensional space that fully reflects the distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "trans_dists = {}\n",
    "for key, value in distances_data.items():\n",
    "    print(key)\n",
    "    trans_dists.update({key: mds.fit_transform(value)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path    \n",
    "import scienceplots\n",
    "\n",
    "def avg_dist_notAQL(x,y, point_matrtix):\n",
    "    dist = 0\n",
    "    for i in range(len(point_matrtix)):\n",
    "        if i != 1:\n",
    "            dist += np.sqrt((x - point_matrtix[i][0])**2 + (y - point_matrtix[i][1])**2)\n",
    "    dist = dist/3\n",
    "    # return dist\n",
    "    # # get the average distance of the points within AOL, ORCAS, MS-MARCO Web Search    \n",
    "    avg_dist_within = 0\n",
    "    for i in [(0,2), (0,3), (2,3)]:\n",
    "        avg_dist_within += np.sqrt((point_matrtix[i[0]][0] - point_matrtix[i[1]][0])**2 + (point_matrtix[i[0]][1] - point_matrtix[i[1]][1])**2)\n",
    "    avg_dist_within = avg_dist_within/3\n",
    "\n",
    "    # get the standard deviation of distances within AOL, ORCAS, MS-MARCO Web Search\n",
    "    std_dist_within = 0\n",
    "    for i in [(0,2), (0,3), (2,3)]:\n",
    "        std_dist_within += (np.sqrt((point_matrtix[i[0]][0] - point_matrtix[i[1]][0])**2 + (point_matrtix[i[0]][1] - point_matrtix[i[1]][1])**2) - avg_dist_within)**2\n",
    "    std_dist_within = np.sqrt(std_dist_within/3) \n",
    "\n",
    "    if dist <= avg_dist_within:\n",
    "        return avg_dist_within\n",
    "    elif dist > avg_dist_within and dist <= avg_dist_within + std_dist_within:\n",
    "        return avg_dist_within + std_dist_within\n",
    "    elif dist > avg_dist_within + std_dist_within and dist <= avg_dist_within + 2 * std_dist_within:\n",
    "        return avg_dist_within + 2 * std_dist_within\n",
    "    elif dist > avg_dist_within + 2 * std_dist_within and dist <= avg_dist_within + 3 * std_dist_within:\n",
    "        return avg_dist_within + 3 * std_dist_within\n",
    "    elif dist > avg_dist_within + 3 * std_dist_within and dist <= avg_dist_within + 4 * std_dist_within:\n",
    "        return avg_dist_within + 4 * std_dist_within\n",
    "    elif dist > avg_dist_within + 4 * std_dist_within and dist <= avg_dist_within + 5 * std_dist_within:\n",
    "        return avg_dist_within + 5 * std_dist_within\n",
    "    else: \n",
    "        return avg_dist_within + 6 * std_dist_within\n",
    "        \n",
    "def avg_dist_within(points):\n",
    "    dist = 0\n",
    "    pairs = [(0, 2), (0, 3), (2, 3)]  # Indizes der Punkte AOL, MS-MARCO, ORCAS\n",
    "    for i, j in pairs:\n",
    "        dist += np.sqrt((points[i][0] - points[j][0])**2 + (points[i][1] - points[j][1])**2)\n",
    "    return dist / len(pairs)\n",
    "def std_within(points):\n",
    "    dist = 0\n",
    "    pairs = [(0, 2), (0, 3), (2, 3)]  # Indizes der Punkte AOL, MS-MARCO, ORCAS\n",
    "    for i, j in pairs:\n",
    "        dist += (np.sqrt((points[i][0] - points[j][0])**2 + (points[i][1] - points[j][1])**2) - avg_dist_within(points))**2\n",
    "    return np.sqrt(dist / len(pairs))\n",
    "\n",
    "# use science style for plots from scienceplots library\n",
    "plt.style.use([\"science\", \"ieee\"])\n",
    "color =  ['tab:blue', 'tab:orange', 'tab:gray', 'tab:red']\n",
    "\n",
    "save_vis = True\n",
    "# save_vis = False\n",
    "\n",
    "# get meshsize\n",
    "# mesh_size = 70\n",
    "mesh_size = 500\n",
    "\n",
    "num_plots = len(trans_dists)\n",
    "cols = 3\n",
    "rows = num_plots//cols\n",
    "if num_plots % cols != 0:\n",
    "    rows+=1\n",
    "\n",
    "textwidth = 5.5129\n",
    "aspect_ratio = 6/8\n",
    "scale = 1.0\n",
    "width = textwidth * scale\n",
    "height = width * aspect_ratio * 0.85\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(rows,cols, figsize=(width,height))\n",
    "\n",
    "\n",
    "# for making the titles\n",
    "keys = [key for key in trans_dists.keys()]\n",
    "\n",
    "titles = [\"Characters per Query\", \"Characters per Entity\", \"Characters per Word\", \"Words per Query\", \"Words per Entity\", \"Entities per Query\"]\n",
    "\n",
    "positions = [[0.0,-1.0], [0.0,-0.6], [0.0,0.0], [-0.4,0.0], [0.04,0.03], [0.0,0.0]]\n",
    "\n",
    "\n",
    "axes = ax.flatten()\n",
    "for i in range(len(trans_dists)):\n",
    "    ax = axes[i]\n",
    "    ax.set_title(titles[i], fontsize=8)\n",
    "\n",
    "    points = trans_dists[keys[i]]\n",
    "\n",
    "    # get the highes and lowest x and y values of the points\n",
    "    x_min = np.min(points[:,0])\n",
    "    x_max = np.max(points[:,0])\n",
    "    y_min = np.min(points[:,1])\n",
    "    y_max = np.max(points[:,1])\n",
    "\n",
    "    # scale the extreme values of x and y to get a good boundary for the plot\n",
    "    # we take the scale of the min range\n",
    "    range_scale = 0.4\n",
    "    x_min = x_min - range_scale * (x_max - x_min)\n",
    "    x_max = x_max + range_scale * (x_max - x_min)\n",
    "    y_min = y_min - range_scale * (y_max - y_min)\n",
    "    y_max = y_max + range_scale * (y_max - y_min)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, mesh_size)\n",
    "    y = np.linspace(y_min, y_max, mesh_size)\n",
    "\n",
    "    z = np.array([avg_dist_notAQL(i,j,points) for j in y for i in x])\n",
    "\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = z.reshape(mesh_size, mesh_size)\n",
    "\n",
    "    avg_dist = avg_dist_within(points)\n",
    "    std_dist = std_within(points)\n",
    "\n",
    "    ax.contourf(X.astype(float), Y.astype(float), Z.astype(float), 10, alpha=0.7, cmap = 'Blues', linewidths=0.5, linestyles='solid') #, cmap='Blues' 'OrRd'\n",
    "    # Add contour lines for the average distance within AOL, MS-MARCO, and ORCAS and the standard deviation \n",
    "    contour = ax.contour(X.astype(float), Y.astype(float), Z.astype(float), levels=[avg_dist, avg_dist+std_dist, avg_dist+2*std_dist, avg_dist+3*std_dist, avg_dist+4*std_dist, avg_dist+5*std_dist], colors='black', linewidths=0.5, linestyles='dashed')\n",
    "\n",
    "    # Label the contour lines with descriptive text\n",
    "    text = [\"$d_{{\\mu}}$\", \"$d_{{\\mu}} + \\sigma$\", \"$d_{{\\mu}} + 2 \\sigma$\"]\n",
    "    for j, level in enumerate(contour.levels):\n",
    "        # Find the position for the label (e.g., the midpoint of the contour line)\n",
    "        idx = np.where(Z == level)\n",
    "        if len(idx[0]) > 0:\n",
    "            if j == 0:\n",
    "                if titles[i] == \"Characters per Entity\":\n",
    "                    x_pos = positions[i][0] \n",
    "                    y_pos = positions[i][1]-0.5\n",
    "                elif titles[i] == \"Words per Query\":\n",
    "                    x_pos = positions[i][0] +0.5\n",
    "                    y_pos = positions[i][1]\n",
    "                elif titles[i] == \"Words per Entity\":\n",
    "                    x_pos = positions[i][0] \n",
    "                    y_pos = positions[i][1] -0.1\n",
    "                else:\n",
    "                    x_pos = positions[i][0] \n",
    "                    y_pos = positions[i][1]\n",
    "                ax.text(x_pos, y_pos, text[j] , fontsize=8, color='black')\n",
    "\n",
    "    labels = [name.upper() for name in names]\n",
    "    labels[2] = \"MS-MARCO WS\"\n",
    "    # plot 3d scatter plot\n",
    "    for j in range(len(points)):\n",
    "        ax.scatter(points[j][0], points[j][1], c=color[j], label=labels[j], s=15, edgecolor='black', linewidth=0.3)\n",
    "\n",
    "    ax.grid(True)\n",
    "    \n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.03), fancybox=False, ncol=len(names),edgecolor=\"black\", frameon=True).get_frame().set_linewidth(0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.9, hspace=0.33, wspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "base_path = \"/home/benjamin/studium/masterarbeit/thesis-schneg/plots/\"\n",
    "\n",
    "vis_dir = Path(f\"{base_path}Wasserstein-Distances-lengths-2d-contour\")\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "if not vis_dir.exists() and save_vis:\n",
    "        vis_dir.mkdir(parents=True)\n",
    "\n",
    "if save_vis:\n",
    "    fig.savefig(vis_dir.joinpath(\"all.pdf\"), format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in trans_dists.items():\n",
    "    print(key)\n",
    "    print(val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

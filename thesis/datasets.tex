\chapter{Data Sets}
In this chapter, we introduce the data sets that we use in our experiments. We provide an overview of the data sets, present their origin and describe possibly needed data cleaning processes that ensure a fair comparison. 
\section{Archive Query Log}
The Archive Query Log (AQL) was published by~\citet{reimer:2023} in order to expand the publically available query logs and promote research in the field of information retrieval. It was mined from the Internet Archive's \textit{Wayback Machine}~\footnote{\url{https://web.archive.org/}} by identifying search engine result pages (SERP) and parsing the queries from the corresponding URLs. \citet{reimer:2023} name two possible reasons why SERPs are archived in the Wayback Machine: First, SERPs can be linked to by web pages and may thus be included in automated web crawls of the Internet Archive. Secondly, any user of the Internet Archive can request archiving a specific URL, hence SERPs may be included, as well. Considering these two cases as the main sources, the AQL's queries would be a mixture of specific queries that users wanted to archive and queries which were linked to by web pages. This might bias the query distribution compared to a real-world search engine query log. The AQL contains around 356.6M queries from the years $1999$ to $2022$. Moreover, it covers a wide range of languages, namely 104 different languages, and a variety of domains. The most prominent domains in the AQL are Baidoo, Google, StackOverflow, Twitter and Youtube. The most frequent query languages in the AQL are English and Cantonese. 

\begin{table}[h]
    \centering
    \scalebox{0.99}{
    \begin{tabular}{@{}lp{0.25\linewidth}lc@{}} \toprule
        \textbf{Column Name} & \textbf{Description} & \textbf{Example} & \textbf{Data Type} \\  \cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-2} \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4}
        \texttt{serp\_query\_text\_url} & Query-string & \texttt{weather} & \texttt{str} \\ 
        \texttt{serp\_timestamp} & Query-timestamp & \texttt{1680604622} & \texttt{int} \\
        \texttt{search\_provider\_name} & The query's search provider & \texttt{google} & \texttt{str} \\
        \texttt{serp\_url} & URL of a query's SERP & \texttt{<SERP-URL>} & \texttt{str} \\
        \midrule
        \midrule
        \textbf{Data Generation} & \multicolumn{3}{l}{SERPs from WayBack Machine} \\
        \textbf{Languages} & \multicolumn{3}{l}{Multilingual, 104 languages} \\
        \textbf{Time Span} & \multicolumn{3}{l}{1999-2022} \\
        \textbf{Num. Rows} & \multicolumn{3}{l}{356.6 M} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Summary of the Archive Query Log.}
    \label{tab:aql-summary}
\end{table}

\subsection{AQL Cleaning}\label{sec:aql-cleaning}
Some results of analysing the AQL have uncovered outliers and further abnormalities that should be removed to ensure a fair comparison between the query logs. First of all, the analysis of the query length distribution has shown that there are some exceptionally frequent lengths in the distribution. In Figure~\ref{fig:aql-character-count-old} we can see that the query length distribution has a peak at the lengths $14$, $16$ and $24$. Therefore, we take a look at the most frequent queries of these lengths. For each length, we find a particularly frequent query whose frequency outnumbers the second most frequent query by several orders of magnitude and likely produces the outlier in the distribution. We remove these uncommonly frequent queries from the data set. Secondly, we noted that a subset of the AQL's queries is subject to a decode error and, as a result, consist of replacement characters. We remove these queries from the data set as well. Lastly, we remove all empty queries, i.e., queries with an empty string, from the data set. 
\begin{figure}[h]
    \centering
    \import{../plots/character-count-frequencies-queries}{aql.pgf}
    \caption{Query length distribution of the AQL before cleaning. Note the peaks at the lengths $14$, $16$ and $24$.}
    \label{fig:aql-character-count-old}
\end{figure}      
% \begin{itemize}
%     \item Herkunft
%     \item Wie wurde es erstellt
%     \item Zeitliche Spanne
%     \item Inhaltlicher Bias
%     \item Größe
%     \item Zu was für einen Zweck wurde es erstellt
%     \item enthaltene Sprachen
%     \item enthaltene Domains
%     \item 
% \end{itemize}



\section{AOL User Session Collection}
The AOL User Session Collection (AOL) was published by~\citet{pass:2006} to provide real query log data based on real users that was intended for research in personalization, query reformulation and other types of search research. It was a dircet publication from AOL to encourage research in the named fields. The dataset contains around 36 million queries from AOL's search engine entered by about 650,000 users. The queries were randomly sampled over a 3-month period from March to May 2006. There were no specific filters applied to the queries, hence the dataset depicts a real-world query distribution. Of the roughly 36 million queries, around 20 million are unique. The language distribution of the queries is highly skewed towards English, as the majority of the queries are in English. The queries are provided with a respective timestamp. Despite the controversial publication of this data set and its violation of user privacy, it remained a useful asset for research in the field of information retrieval~\citep{macavaney:2022}. 
\begin{table}[h]
    \centering
    \scalebox{0.99}{
    \begin{tabular}{@{}lp{0.25\linewidth}lc@{}} \toprule
        \textbf{Column Name} & \textbf{Description} & \textbf{Example} & \textbf{Data Type} \\  \cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-2} \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4}
        \texttt{serp\_query\_text\_url} & Query-string & \texttt{weather} & \texttt{str} \\ 
        \texttt{serp\_id} & Query-ID & \texttt{1733} & \texttt{int} \\
        \texttt{serp\_timestamp} & Query-timestamp & \texttt{(2006,3,2)} & \texttt{datetime} \\
        \texttt{serp\_offset} & Rank of clicked URL  & \texttt{3} & \texttt{int} \\
        \midrule
        \midrule
        \textbf{Data Generation} & \multicolumn{3}{l}{Random sample over a 3-month period} \\
        \textbf{Languages} & \multicolumn{3}{l}{Mostly english} \\
        \textbf{Time Span} & \multicolumn{3}{l}{May, 2006} \\
        \textbf{Num. Rows} & \multicolumn{3}{l}{36 M.} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Summary of the AOL Log.}
    \label{tab:aol-summary}
\end{table}

\section{MS-MARCO Web Search}
The MS-MARCO Web Search dataset, published by~\citet{chen:2024}, contains around 10 million queries from 93 different languages. The queries were mined from Microsoft's Bing search engine. According to the publishers, the dataset closely mimics real-world web document and query distributions and was created to serve as a critical data foundation for future research in downstream tasks of information retrieval. The dataset was constructed by filtering the Bing query log from a selected time span for queries that have a click connection to the ClueWeb22 document set. ClueWeb22 is a large dataset of web documents and is aligned with the document distributions in commercial web search~\citep{overwijk:2022}. Hence, the publishers expect the query distribution of the MS-MARCO Web Search dataset to mimic real-world distributions, as well. The query set was further filtered to remove queries that are rarely triggered, contain personally identifiable information, offensive content or adult content. By rarely triggered the authors mean that queries are removed if they were triggered by less than K users, where K is a high number. Moreover, the dataset contains only unique queries. As a result, the query distribution would be slightly different from the real web query distribution but would still closely resemble it. Consequently, the query language distribution also mimics the real-world language distribution of the web. The most frequent language in the dataset is English, followed by Chinese, Japanese, German, French and Spanish.  
\begin{table}[h]
    \centering
    \begin{tabular}{@{}lp{0.2\linewidth}lc@{}} \toprule
        \textbf{Column Name} & \textbf{Description} & \textbf{Example} & \textbf{Data Type} \\  \cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-2} \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4}
        \texttt{serp\_query\_text\_url} & Query-string & \texttt{JFK airport} & \texttt{str} \\ 
        \texttt{serp\_id} & Query-ID & \texttt{1733} & \texttt{int} \\
        \texttt{language} & Query-language & \texttt{en-US} & \texttt{int} \\
        \midrule
        \midrule
        \textbf{Data Generation} & \multicolumn{3}{p{0.55\linewidth}}{Bing queries with a clicked connection to ClueWeb22} \\
        \textbf{Languages} & \multicolumn{3}{l}{Multilingual, 93 languages} \\
        \textbf{Time Span} & \multicolumn{3}{l}{-} \\
        \textbf{Num. Rows} & \multicolumn{3}{l}{10 M.} \\
        \bottomrule
    \end{tabular}
    \caption{Summary of the MS-MARCO Web Search Log.}
    \label{tab:ms-marco-summary}
\end{table}

\section{ORCAS}
The Open Resource for Click Analysis in Search (ORCAS) dataset was provided by~\citet{craswell:2020}. It is, like MS-MARCO Web Search, based on a subsample of Bing's query logs. In this case, the subsample stems from a 26-month period up to January 2020. Queries of this dataset were aggregated based on a click-connection to the TREC Deep Learning documents. The queries of TREC DL were selected in a way that favored natural language questions. Since ORCAS' queries were selected based on a connection to the TREC DL documents, they still might be biased towards questions but, according to~\citet{craswell:2020}, also have words in the top-10 that are more rare in TREC DL such as ``www'', ``the'', ``best'' and ``free''. The queries of ORCAS were filtered for potentially offensive queries, like queries related to hate speech or pornography, and for queries that had very negative post-click signals, such as a short dwell time. Additionally, only english queries which were typed by K different users from the United States for a high value of k were kept in the dataset. The dataset contains around 10 million unique queries and is intended for web mining, query autocompletion and ranking tasks.   
\begin{table}[h]
    \centering
        \begin{tabular}{@{}lp{0.25\linewidth}lc@{}} \toprule
            \textbf{Column Name} & \textbf{Description} & \textbf{Example} & \textbf{Data Type} \\  \cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-2} \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4}
            \texttt{serp\_query\_text\_url} & Query-string & \texttt{``weather''} & \texttt{str} \\ 
            \texttt{serp\_id} & Query-ID & \texttt{120133} & \texttt{int} \\
            \midrule
            \midrule
            \textbf{Data Generation} & \multicolumn{3}{p{0.6\linewidth}}{Bing queries with a clicked connection to TREC Deep Learning documents} \\
            \textbf{Languages} & \multicolumn{3}{l}{English} \\
            \textbf{Time Span} & \multicolumn{3}{l}{26-month period up to January, 2020} \\
            \textbf{Num. Rows} & \multicolumn{3}{l}{10 M.} \\
            \bottomrule
        \end{tabular}
    \caption{Summary of ORCAS.}
    \label{tab:orcas-summary}
\end{table}

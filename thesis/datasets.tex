\chapter{Data Sets}
In this chapter, we introduce the data sets that we use in our experiments. We provide a overview of the data sets, present their origin and describe possibly needed data cleaning processes that ensure a fair comparison. 
\section{Archive Query Log}
The Archive Query Log (AQL) was published by~\citet{reimer:2023} in order to expand the publically available query logs and promote research in the field of information retrieval. It was mined from the Internet Archive's \textit{Wayback Machine}~\footnote{\url{https://web.archive.org/}} by identifying search engine result pages (SERP) and parsing the queries from the corresponding URLs. \citet{reimer:2023} name two possible reasons why SERPs are archived in the Wayback Machine: First, SERPs can be linked to by web pages and may thus be included in automated web crawls of the Internet Archive. Secondly, any user of the Internet Archive can request archiving a specific URL, hence SERPs may be included, as well. Considering these two cases as the main sources, the AQL's queries would be a mixture of specific queries that users wanted to archive and queries which were linked to by web pages. This might bias the query distribution compared to a real-world search engine query log. The AQL contains around 356.6M queries from the years $1999$ to $2022$. Moreover, it covers a wide range of languages, namely 104 different languages, and a variety of domains. The most prominent domains in the AQL are Baidoo, Google, StackOverflow, Twitter and Youtube. The most frequent query languages in the AQL are English and Cantonese.  
\subsection{AQL Cleaning}
Some results of analysing the AQL have uncovered outliers and further abnormalities that should be removed to ensure a fair comparison between the query logs. First of all, the analysis of the query length distribution has shown that there are some exceptionally frequent lengths in the distribution. In Figure~\ref{fig:aql-character-count-old} we can see that the query length distribution has a peak at the lengths $14$, $16$ and $24$. Therefore, we take a look at the most frequent queries of these lengths. For each length, we find a particularly frequent query whose frequency outnumbers the second most frequent query by several orders of magnitude and likely produces the outlier in the distribution. We remove these uncommonly frequent queries from the data set. Secondly, we noted that a subset of the AQL's queries is subject to a decode error and, as a result, consist of replacement characters. We remove these queries from the data set as well. Lastly, we remove all empty queries, i.e., queries with an empty string, from the data set. 
\begin{figure}[h]
    \centering
    \import{../plots/character-count-frequencies-queries}{aql.pgf}
    \caption{Query length distribution of the AQL before cleaning. Note the peaks at the lengths $14$, $16$ and $24$.}
    \label{fig:aql-character-count-old}
\end{figure}      
% \begin{itemize}
%     \item Herkunft
%     \item Wie wurde es erstellt
%     \item Zeitliche Spanne
%     \item Inhaltlicher Bias
%     \item Größe
%     \item Zu was für einen Zweck wurde es erstellt
%     \item enthaltene Sprachen
%     \item enthaltene Domains
%     \item 
% \end{itemize}



\section{AOL User Session Collection}
The AOL User Session Collection (AOL) was published by~\citet{pass:2006} to provide real query log data based on real users that was intended for research in personalization, query reformulation and other types of search research. It was a dircet publication from AOL to encourage research in the named fields. The dataset contains around 36 million queries from AOL's search engine entered by about 650,000 users. The queries were randomly sampled over a 3-month period from March to May 2006. There were no specific filters applied to the queries, hence the dataset depicts a real-world query distribution. Of the roughly 36 million queries, around 20 million are unique. The language distribution of the queries is highly skewed towards English, as the majority of the queries are in English. The queries are provided with a respective timestamp. Despite the controversial publication of this data set and its violation of user privacy, it remained a useful asset for research in the field of information retrieval~\citep{macavaney:2022}. 

\section{MS-MARCO Web Search}
The MS-MARCO Web Search dataset, published by~\citet{chen:2024}, contains around 10 million queries from 93 different languages. The queries were mined from Microsoft's Bing search engine. According to the publishers, the dataset closely mimics real-world web document and query distributions and was created to serve as a critical data foundation for future research in downstream tasks of information retrieval. The dataset was constructed by filtering the Bing query log from a selected time span for queries that have a click connection to the ClueWeb22 document set. ClueWeb22 is a large dataset of web documents and is aligned with the document distributions in commercial web search~\citep{overwijk:2022}. Hence, the publishers expect the query distribution of the MS-MARCO Web Search dataset to mimic real-world distributions, as well. The query set was further filtered to remove queries that are rarely triggered, contain personally identifiable information, offensive content or adult content. By rarely triggered the authors mean that queries are removed if they were triggered by less than K users, where K is a high number. Moreover, the dataset contains only unique queries. As a result, the query distribution would be slightly different from the real web query distribution but would still closely resemble it. Consequently, the query language distribution also mimics the real-world language distribution of the web. The most frequent language in the dataset is English, followed by Chinese, Japanese, German, French and Spanish.  
% \begin{itemize}
%     \item 10 mio unique queries from 93 languages from Bing search engine
%     \item a query set that reflects the real web query distribution
%     \item the first largescale information-rich web dataset with millions of real clicked query-document labels
%     \item As the first large, real and rich web dataset, MS MARCO Web Search will serve as a critical data foundation for future AI and systems research.
%     \item This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models
%     \item As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research
%     \item initial query set gets filtered to remove queries that are rarely triggered, contain personally identifiable information offensive content, adult content and those having no click connection to the ClueWeb22 document set.
%     \item To protect user privacy and content health, we remove queries
%     that are rarely triggered (triggered by less than K users, where K is a high value), contain personally identifiable information offensive content, adult content and queries that have no click connection to the ClueWeb22 document set. As a result, the query distribution is slightly different from the real web query distribution.
%     \item Creation: they take ClueWeb22 documents and filter their query log from a selected time span for queries that have a click connection to the ClueWeb22 documents.
%     \item languages: distribution mimics the real-world language distribution of the crawled web by search engines. 
%     \item english by far most frequent language, then chinese, japanese, german, french, spanish --> distribution is highly skewed
% \end{itemize}
\section{ORCAS}
The Open Resource for Click Analysis in Search (ORCAS) dataset was provided by~\citet{craswell:2020}. It is, like MS-MARCO Web Search, based on a subsample of Bing's query logs. In this case, the subsample stems from a 26-month period up to January 2020. Queries of this dataset were aggregated based on a click-connection to the TREC Deep Learning documents. The queries of TREC DL were selected in a way that favored natural language questions. Since ORCAS' queries were selected based on a connection to the TREC DL documents, they still might be biased towards questions but, according to~\citet{craswell:2020}, also have words in the top-10 that are more rare in TREC DL such as ``www'', ``the'', ``best'' and ``free''. The queries of ORCAS were filtered for potentially offensive queries, like queries related to hate speech or pornography, and for queries that had very negative post-click signals, such as a short dwell time. Additionally, only english queries which were typed by K different users from the United States for a high value of k were kept in the dataset. The dataset contains around 10 million unique queries and is intended for web mining, query autocompletion and ranking tasks.   
% \begin{itemize}
%     \item 10 mio unique queries
%     \item Open Resource for Click Analysis in Search (ORCAS).
%     \item queries that have been repeated across many users to meet privacy concerns
%     \item  ORCAS data has clicks for TREC documents only, and only for English-speaking users in the United States
%     \item ORCAS dataset was created through log mining, aggregation and filtering
%     \item we apply some minimal filtering, to eliminate clicks that had very negative post-click signals, such as a short dwell, since this can indicate that the user was disappointed with the result
%     \item aggregated based on a subsample of Bing’s 26-month logs to January 2020
%     \item only keep query-URL pairs where the URL is present in the 3.2 million document TREC DL corpus.
%     \item keeping only queries that were typed by k different users, for a high value of k
%     \item applied filters to remove potentially offensive queries, for example queries related to pornography or hate speech
%     \item The requirement is that the same ranking was seen by k users who all selected the same URL. Because rankings vary from user to user and also vary over time, very few cases reach the k threshold.
%     \item  TREC DL queries were selected in a way that favored natural language questions, so have less variety of first words, and more focus on question words. ORCAS queries were selected based on the TREC DL documents, so still have question words, but also have words in the top-10 that are more rare in TREC DL such as www, the, best and free.
%     \item ORCAS queries were selected based on connection to the TREC
%     corpus, click aggregation, anonymity filtering and other filtering
%     criteria. They were not otherwise selected to be natural language
%     question answering queries. By contrast, the TREC DL queries were selected in the creation of the MS MARCO question answering task
%     \item the dataset's use case is web mining, query autocompletion and ranking tasks
% \end{itemize}

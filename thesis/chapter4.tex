
\chapter{Analysis/Methodology}  

\section{Descriptive Statistics/ Text-related statistics}
In this section, we generate a set of descriptive/text-related statistics from the query logs. The goal is to perform a comparison of the query logs' linguistic and structural composition, initially neglecting semantical information of the queries.    
Specifically, we perform a hypothesis test to determine if the syntactical differences between the AQL and the other query logs are statistically significant. For this analysis, we look at queries from different syntactic perspectives and carry out measurements in the defined perspectives. We define the following categories as syntactic perspectives:
\begin{enumerate}
    \item Queries
    \item Named Entities
    \item Words
    \item Characters
\end{enumerate} 
Even though named entities are not considered a syntactic category primarily, we include them in this analysis since they are frequent enough to be regarded a structural element of queries. Hence, their analysis might provide an additional valuable insight to the structure of query logs. For each of the aforementioned categories, we carry out two types of measurements: First, we collect all existing items of the category in a query log and determine the frequency of each item. Secondly, we measure the lengths of all extracted items in terms of all possible subcategories. The defined syntactic categories are subject to a hierachical order, i.e., queries can be described as a set of named entities, words or characters. Words, in turn, can not be described as a set of named entities. Accordingly, we measure lengths of queries in terms of named entities, words and characters. Named entities are measured by the count of words or characters. By continuing this procedure for all categories we gain a thorough set of measurements for each query log. The resulting distributions of the measurements are then compared to each other. Essentially, we test the AQL's distributions for an equality of distribution with the other query logs to judge the AQL's authenticity.

\subsection{Category-Related Frequencies}
For each query log, we extract all elements of a category and determine the frequency of each element. For instance, we extract all exisiting words from the query log and measure each word's frequency. The frequency distribution of linguistic elements typically obey Zipf's law when ordered in a descending order. Especially the frequency distribution of words is well studied and a popular example for Zipf's law~\citep{piantadosi:2014}. Zipf's law states that the frequency $f$ of an element is inversely proportional to its rank $r$ in the frequency table with some scaling constant $c$ and exponent $\alpha \approx 1$:
\begin{equation}
    f \propto \frac{c}{r^{\alpha}}
\end{equation} 
The law requires the frequencies to be ordered in a descending order, hence it describes frequencies relative to the \textit{rank} of elements. We test the frequencies of queries, named entities, words and characters for Zipf's law by sorting them in descending order and displaying them in a log-scaled graph. Albeit primarily studied for words, we attempt on retrieving Zipf's law also in the frequencies of queries, named entities and characters since they as well are linguistic categories and probably follow linguistic dynamics. 
\begin{figure}
    \centering
    \import{../plots/extract-named-entities-and-extract-words-single}{all.pgf}
    \caption{Frequency of named entities and words in the query logs.}
\end{figure} 

\section{Characteristics-based}
Here, various characteristics of the data sets are measured and displayed. The comparison of these characteristics should provide a first impression of the involved query logs' nature.

\begin{itemize}
    \item Query length 
    \begin{itemize}
        \item Length in words 
        \begin{itemize}
            \item mapping->group->reduction
        \end{itemize} 
        \item Length in characters \begin{itemize}
            \item mapping->group->reduction
        \end{itemize} 
        \item Plot histogram
    \end{itemize}
    \item Unique queries
    \begin{itemize}
        \item Proportion of unique queries group by query
        \item List of top n most common queries
        \begin{itemize}
            \item group->reduction->sort
        \end{itemize} 
        \item Compare most common queries
    \end{itemize}

    \item Token frequency
    \begin{itemize}
        \item Zipf's Law of words and/or queries
        \begin{itemize}
            \item Words:  flat mapping->group->reduction
        \end{itemize}
        \begin{itemize}
            \item Queries: group->reduction
            \item character-based (evtl. anderes gesetz)
            \item total number of characters
        \end{itemize}
        \item Heap's Law
        \begin{itemize}
            \item group->reduction
        \end{itemize} 
    \end{itemize}
    \item Search Operators
    \begin{itemize}
        \item Frequency of operators
        \begin{itemize}
            \item flat mapping->group by
        \end{itemize}
        \item most common operators
        \item phrasensuche, site:
    \end{itemize}
    \item Word frequencies
    \begin{itemize}
        \item most common words, bigrams, n-grams
        \item list of top n common words, bigrams, n-grasms
        \item flat_mapping (apply tokenizer) -> group-by token -> count
        \item total number of words
    \end{itemize}
    \item Word lengths
    \begin{itemize}
        \item can be measured during word-frequency-experiment?
        \item average word length, deviation, distribution of word lengths? 
    \end{itemize}
    
\end{itemize} 



\section{Distribution-based}
In this section, distributions of labels are created from different NLP predictors. The general idea is to compare the different data sets according to their resulting distributions of different domains.  
\begin{itemize}
    \item Plausibilitätsstudie für die classifier: 50-100 samples pro label pro classifier: manuell annotieren und dann accuracy bestimmen.
    \item Intent
    \begin{itemize}
        \item Navigational, transactional and informational
        \begin{itemize}
            \item mapping (apply ORCAS-I classifier)
        \end{itemize}
    \end{itemize}
    \item Named Entities
    \begin{itemize}
        \item Frequency of named entities \begin{itemize}
            \item flat mapping->group->reduction
        \end{itemize} 
        \item Most common named entities
        \item Categorize named entities according to ... -> apply classifier?
    \end{itemize}
    \item Hate speech
    \begin{itemize}
        \item mapping (apply hate speech classifier)
    \end{itemize}
    \item NSFW
    \begin{itemize}
        \item mapping (apply NSFW classifier)
    \end{itemize}
    \item Spam
    \begin{itemize}
        \item mapping (apply spam classifier)
    \end{itemize}
    \item Content
    \begin{itemize}
        
        \item Classify into topic taxonomy 
        \begin{itemize}
            \item mapping apply topic classifier
        \end{itemize}
    \end{itemize}
    
\end{itemize}
\section{Temporal-based}
\begin{itemize}
    \item Discover Google Trends in Data
    \item Plot frequency of selected topics over time per data set
    \item Seasonal topics

    
\end{itemize}
\section{Embedding-based}
\begin{itemize}
    \item Extract document vectors from e.g. BERT
    \begin{itemize}
        \item Create t-SNE plot with regard to intent, topics,... further categories
        \item Apply clusterting algorithm and compare resulting clustersX 
    \end{itemize}
    \item Topic Modeling
        \begin{itemize}
            \item Compare most common topics
            \item Compare variety of topics
        \end{itemize}
\end{itemize}


